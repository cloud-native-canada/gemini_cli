{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hands-on with Gemini CLI","text":"<p>Last updated: Sep 17, 2025</p>"},{"location":"#1-introduction","title":"1. Introduction","text":"<p>In this Lab, you will learn about Gemini CLI, an open-source AI agent that brings the power of Gemini directly into your terminal.</p> <p>The Gemini CLI project is: </p> <ul> <li>A lightweight, powerful, and accessible CLI tool that integrates cutting-edge AI directly into the terminal</li> <li>It is a powerful AI agent that automates tasks, write code, and debug issues.  </li> <li>Connects to Google Search and MCP servers like GitHub.</li> <li>Gemini CLI is open-source and you can view the public roadmap here.</li> </ul>"},{"location":"#what-youll-learn","title":"What you'll learn","text":"<ul> <li>Installing and configuring Gemini CLI</li> <li>Exploring tools, built-in commands and configuring MCP Servers in Gemini CLI</li> <li>Customizing Gemini CLI via the <code>GEMINI.md</code> file</li> <li>Exploring a few use cases with Gemini CLI</li> </ul>"},{"location":"#what-youll-need","title":"What you'll need","text":"<p>This lab can be run entirely within Google Cloud Shell, which comes pre-installed with Gemini CLI.</p> <p>Alternatively, if you prefer to work on your own machine, there is a section to install Gemini CLI locally.</p> <p>You would need the following:</p> <ul> <li>Chrome web browser</li> <li>A Gmail account</li> </ul> <p>This lab is designed for users and developers of all levels (including beginners). The use cases in the lab have been categorized into developer and non-developer tasks. The developer use cases demonstrate how to vibe code with Gemini CLI and working with a Github repository to perform common development tasks like code explanation/understanding, generating documentation, fixing issues and more. It is recommended that you complete these use cases in the lab. There is an optional section at the end that covers several every day tasks that are non-developer focused.</p>"},{"location":"0_install_cli/","title":"Installation","text":""},{"location":"0_install_cli/#1-gemini-cli-installation","title":"1. Gemini CLI Installation","text":"<p>Before you do the setup and run Gemini CLI, let us create a folder that you will be using as our home folder for all the projects that you may create inside of it. This is a starting point for the Gemini CLI to work with, though it will also reference some other folders on your system and which you will come to later, as needed.</p> <p>Go ahead and create a sample folder (<code>gemini-cli-projects</code>) and navigate to that via the commands shown below. If you prefer to use some other folder name, please do so.</p> <pre><code>mkdir gemini-cli-projects\n</code></pre> <p>Let's navigate to that folder:</p> <pre><code>cd gemini-cli-projects\n</code></pre> <p>Note: If you are using Google Cloud Shell, the Gemini CLI is already installed. You can directly launch Gemini CLI via the <code>gemini</code> command. Please navigate directly to the next section (Gemini CLI configuration via settings.json).</p> <p>If you want to install Gemini CLI locally, follow the instructions given below.</p>"},{"location":"0_install_cli/#install-node-20","title":"Install Node 20 +","text":"<p>The first step is to install Node 20+ on your machine. Once this is complete, you can install and run Gemini CLI via any one of the following methods:</p>"},{"location":"0_install_cli/#install-gemini-cli","title":"Install Gemini CLI","text":"<p>You can install Gemini CLI globally on your system first. You may need Administrator access to perform this step.</p> Apple Mac OsX / Linux / Windows"},{"location":"0_install_cli/#run-instantly-with-npx","title":"Run instantly with npx","text":"<pre><code># Using npx (no installation required)\nnpx https://github.com/google-gemini/gemini-cli\n</code></pre>"},{"location":"0_install_cli/#install-globally-with-npm","title":"Install globally with npm","text":"<pre><code>npm install -g @google/gemini-cli\n</code></pre> <p>Login with Google (OAuth login using your Google Account)</p> <p>\u2728 Important step: For anyone who has a Gemini Code Assist License, make sure project with licenses is set</p> Apple Mac OsX / Linux <p>Option 1: Local variable</p> <pre><code># Set your Google Cloud Project\nexport GOOGLE_CLOUD_PROJECT=\"YOUR_PROJECT_NAME\"\n</code></pre> <p>Option 2: Set a persistent environment variable in your  <code>bash</code> or <code>zshrc</code> shell:</p> <pre><code>echo 'export GOOGLE_CLOUD_PROJECT=\"YOUR_PROJECT_NAME\"' &gt;&gt; ~/.zshrc\n</code></pre> <pre><code>echo 'export GOOGLE_CLOUD_PROJECT=\"YOUR_PROJECT_NAME\"' &gt;&gt; ~/.bashrc\n</code></pre> <p>Note: Make sure update \"YOUR_PROJECT_NAME\" wit project with Gemini Code Assist License.</p> Windows <p>Option 1. local Environment variable</p> <pre><code># Set your Google Cloud Project\nset GOOGLE_CLOUD_PROJECT=\"YOUR_PROJECT_NAME\"\n</code></pre> <p>Option 2. Set a persistent environment variable on Windows using the System Properties (sysdm.cpl) GUI</p> <pre><code>- Press Win + R to open the \"Run\" dialog box.\n- Type sysdm.cpl and press Enter. This will open the \"System Properties\" window.\n- Navigate to the \"Advanced\" tab.\n- Click on the \"Environment Variables...\" button.\n- In the \"User variables\" section at the top, click the \"New...\" button.\n- In the \"New User Variable\" dialog:\n  - For Variable name, enter: GOOGLE_CLOUD_PROJECT\n  - For Variable value, enter your actual Google Cloud project name (e.g., my-gcp-project).\n- Click \"OK\" to close the \"New User Variable\" dialog.\n</code></pre> <pre><code># .. and then run\ngemini\n</code></pre> <p>You can confirm the CLI is installed by running:</p> <pre><code>gemini --version\n</code></pre> <p>Assuming that you have launched Gemini CLI first time you should see the following screen that asks you about choosing a a theme. Go ahead and select one that you like:</p> <p></p> <p>Once you select that, it will ask for the Authentication method. If you are using Gemini Code Assist License, it is recommended that you use  <code>1. Login with Google</code>.</p> <p></p> <p>Go ahead and click on Enter. This will open up a Google Authentication page in the browser. Go ahead with the authentication with your Google Account, accept the terms and once you are successfully authenticated, you will notice that the Gemini CLI is ready and waiting for your command. A sample screenshot is given below:</p> <p></p>"},{"location":"0_install_cli/#first-interaction-with-gemini-cli","title":"First interaction with Gemini CLI","text":"<p>Let's get started with Gemini CLI and type your first query as shown below:</p> <pre><code>Give me a famous quote on Artificial Intelligence and who said that?\n</code></pre> <p>The expected response is shown below:</p> <p></p> <p>You will notice that our query resulted in a <code>GoogleSearch</code> tool (an in-built tool in Gemini CLI) that got invoked. In other words, you have already exercised one of Gemini CLI's powerful in-built tools namely <code>GoogleSearch</code> that grounds its responses based on information that it gets from the web. You will learn more about Tools in the next section.</p> <p>A quick way to understand Gemini CLI and the various commands that it supports is to type <code>/help</code> (forward slash) and you will see a variety of commands and keyboard shortcuts as shown below:</p> <p></p> <p>Let's quit Gemini CLI for now. You can do that either via the <code>/quit</code> command or you can do <code>Ctrl-C</code> twice in the interactive Gemini CLI terminal session.</p>"},{"location":"1_cli_params/","title":"Gemini CLI - Command Parameters","text":""},{"location":"1_cli_params/#gemini-cli-command-parameters","title":"Gemini CLI - Command Parameters","text":"<p>There are a few command line parameters that one can provide when you start Gemini CLI. To get a full list of options, you can use the <code>--help</code> as shown below.</p> <pre><code>gemini --help\n</code></pre> <p>This should show the full range of options available. You are encouraged to go through the documentation here.</p> <p>Let us take a look at a few of them. The first one is to configure Gemini CLI to use either the Pro or the Flash model. Currently, at the time of writing this lab, these are the only two models supported. By default the Gemini 2.5 Pro model is used, but if you would like to use the Flash Model, you can do that at the time of starting Gemini CLI via the <code>-m</code> parameter as shown below:</p> <pre><code># gemini -m \"gemini-2.5-flash\"\n</code></pre> <p>For this Labs we will be using gemini-2.5-pro. So make sure to set it back:</p> <pre><code>gemini -m \"gemini-2.5-pro\"\n</code></pre> <p>You will notice that if you start in the above manner, you can check the model at the bottom right of the Gemini CLI terminal as shown below:</p> <p></p>"},{"location":"1_cli_params/#non-interactive-mode","title":"Non-interactive mode","text":"<p>An interesting option is to run Gemini CLI in a non-interactive mode. This means that you directly provide it the prompt and it will go ahead and respond to it, without the Gemini CLI interactive terminal opening up. This is very useful if you plan to use Gemini CLI in an automated fashion as part of the script or any other automation process. You use the <code>-p</code> parameter to provide the prompt to Gemini CLI as shown below:</p> <pre><code>gemini -p \"What is the gcloud command to deploy to Cloud Run\"\n</code></pre> <p>Do keep in mind that there is no scope to continue the conversation with follow up questions. This mode also does not allow you to authorise tools (including <code>WriteFile</code>) or to run shell commands.</p>"},{"location":"2_cli_vibecoding/","title":"Gemini CLI - Vibecoding","text":""},{"location":"2_cli_vibecoding/#what-is-vibecoding","title":"What is Vibecoding?","text":"<p>Vibe coding refers to the practice of instructing AI agents to write code based on natural language prompts.</p> <p>It's not about being lazy\u2014it's about focusing your time and energy on the creative aspects of app development rather than getting stuck in technical details.</p> <p>At its core, vibe coding is about communicating with AI in natural language to build apps. Instead of writing code, you describe what you want your app to do, and AI tools handle the technical implementation. </p>"},{"location":"2_cli_vibecoding/#task-1-create-a-new-app","title":"Task 1 Create a new app","text":"<p>Let's give Gemini CLI our first task and ask it to build a web application that displays the content of a current RSS feed. Say I follow Cricket with keen interest and for some reason, I am interested in viewing live scores of any match that is going on. Not sure why, but let's leave that here.</p> <p>Cricinfo.com provides a live feed of cricket scores over here: <code>https://static.cricinfo.com/rss/livescores.xml</code>. Let's put Gemini CLI to the test and see how well it does on a sample web application that I'd like to create. Let's go step by step.</p> <p>Step 1: Execute the following prompt in Gemini CLI:</p> <pre><code>I would like to create a Python Flask Application that shows me a list of live scores of cricket matches.\nThere is a RSS Feed for this that is available over here: `https://static.cricinfo.com/rss/livescores.xml`. Let's use that.\n</code></pre> <p>Gemini CLI responds with the following:</p> <p></p> <p>You can see that it is planning to use one of the tools to create a directory, for which it needs permission to run the <code>mkdir</code> command. This is a recurrent pattern that you will see, where you can either allow the tool once, always allow and so on. I am going to allow these tools for now since it is being transparent with what it is going to create, i.e. a sub-folder named <code>cricket-scores-app</code> in my <code>gemini-cli-projects/cli-series</code>, which is the folder from which I launched the Gemini CLI.</p> <p>Once it created the folders and touched the files required, it is now beginning to generate code for the application and is using the <code>WriteFile</code> tool to create the file + contents, for which it needs permission too.</p> <p></p> <p>Its then gone ahead and created the Python file, HTML file, its identified the Python requirements (<code>Flask</code>, <code>requests</code> and <code>feedparser</code> packages) and is now asking me if it can install the Python packages via <code>pip</code>.</p> <p></p> <p>It then goes ahead and creates a virtual environment in Python to setup the dependencies. I give it the permissions and it goes about merrily with the next steps to install the dependencies in the environment and starting the Flask Server on port 5000. It gave an error saying that port 5000 is already in use, so I hit <code>ESC</code> and asked it to modify the server code to run on port 7000 instead.</p> <p></p> <p>Once the changes were done, it was able to launch the server successfully.</p> <p></p> <p>Step 2:  Check the application locally at <code>http://127.0.0.1:7000</code>, a screenshot of which is shown below:</p> <p></p> <p>The code generated for the Flask Application was simple and straightforward:</p> <p>app.py <pre><code>import flask\nimport feedparser\nimport requests\n\napp = flask.Flask(__name__)\n\ndef get_live_scores():\n  \"\"\"Fetches and parses live cricket scores from the RSS feed.\"\"\"\n  rss_url = \"https://static.cricinfo.com/rss/livescores.xml\"\n  try:\n    response = requests.get(rss_url)\n    response.raise_for_status() # Raise an exception for bad status codes\n    feed = feedparser.parse(response.content)\n    return feed.entries\n  except requests.exceptions.RequestException as e:\n    print(f\"Error fetching RSS feed: {e}\")\n    return []\n\n@app.route('/')\ndef index():\n  \"\"\"Renders the index page with live scores.\"\"\"\n  scores = get_live_scores()\n  return flask.render_template('index.html', scores=scores)\n\nif __name__ == '__main__':\n  app.run(debug=True, port=7000)\n</code></pre></p> <p>templates/index.html <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\"&gt;\n  &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n  &lt;title&gt;Live Cricket Scores&lt;/title&gt;\n  &lt;link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;div class=\"container\"&gt;\n    &lt;h1 class=\"mt-5\"&gt;Live Cricket Scores&lt;/h1&gt;\n    &lt;ul class=\"list-group mt-3\"&gt;\n      {% for score in scores %}\n        &lt;li class=\"list-group-item\"&gt;{{ score.title }}&lt;/li&gt;\n      {% endfor %}\n    &lt;/ul&gt;\n  &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>We have hardly scratched the surface here but feel free to take it for a spin. Try out some tasks or an application or two that you would like to see getting generated. It's early days, so be ready for some surprises. I have found that I usually end up prompting again with more context or asking it to do better and more.</p> <p>As we progress through this lavs, we will see a lot more of configuration, context setting and then when we come to various use cases, we will look at developing applications from scratch, migrating current applications, adding a feature or two to an existing application and more. So stay tuned.</p>"},{"location":"2_cli_vibecoding/#ai-coding-best-practices","title":"AI Coding Best Practices","text":"<p>Be precise: Avoid open-ended prompts when guiding AI agents. Instead, provide clear and specific directions about what you want to achieve. This allows the AI to more accurately translate your vision into code and reduces unnecessary iterations or misunderstandings.</p> <p>Give agents one task at a time: Understand the limitations of AI models' context windows, meaning they can only handle a finite amount of information at once. Break down your requirements into small, focused tasks, ensuring the AI has only the relevant context it needs, and consider providing additional context incrementally to maintain clarity and efficiency.</p> <p>Use checkpoints: Implement checkpoints or version control regularly to capture stable states of your project. This practice allows you to quickly roll back to a functional version if introducing a new feature inadvertently disrupts your existing setup, significantly simplifying the debugging process.</p> <p>Ask a lot of questions: Engage actively with your AI tool by frequently asking clarifying questions, such as how best to solve a particular problem or what frameworks and technologies might be most suitable for your goals. Additionally, asking questions about how specific parts of your app function can be particularly valuable when debugging, helping you systematically trace and resolve errors.</p>"},{"location":"3_cli_config/","title":"Gemini CLI - Configuration via settings.json","text":""},{"location":"3_cli_config/#gemini-cli-configuration-via-settingsjson","title":"Gemini CLI configuration via settings.json","text":"<p>If you choose Cloud Shell to run Gemini, a default theme for Gemini CLI and the authentication method is already selected and configured for you.</p> <p>If you installed Gemini CLI on your machine and launched it for the first time, you selected a theme and then an authentication method.</p> <p>Now, on subsequent runs of Gemini CLI, you will not be asked to select a theme and authentication method again. This means that it is getting persisted somewhere and the file that it uses is called <code>settings.json</code> and it is the way to customize Gemini CLI.</p> <p>Settings are applied with the following precedence (Cloud Shell only makes User settings available):</p> Linux <ul> <li>System: <code>/etc/gemini-cli/settings.json</code> (applies to all users, overrides user and workspace settings).</li> <li>Workspace: <code>.gemini/settings.json</code> (overrides user settings).</li> <li>User: <code>~/.gemini/settings.json</code>.</li> </ul> Windows <ul> <li>Windows<ul> <li>User: <code>%USERPROFILE%\\.gemini\\settings.json</code> (which typically expands to <code>C:\\Users\\&lt;YourUsername&gt;\\.gemini\\settings.json</code>)</li> <li>System: <code>%ProgramData%\\gemini-cli\\settings.json</code> (which typically expands to <code>C:\\ProgramData\\gemini-cli\\settings.json</code>)</li> </ul> </li> </ul> Apple Mac OsX  <ul> <li>User: <code>~/.gemini/settings.json</code> (which expands to <code>/Users/&lt;YourUsername&gt;/.gemini/settings.json</code>)</li> <li>System: <code>/etc/gemini-cli/settings.json</code></li> </ul> <p>If you recollect, at the time of selecting the theme, you selected for the settings to be saved in the User Settings. So visit the <code>~/.gemini</code> folder and you will notice the <code>settings.json</code> file.</p> <p>My <code>settings.json</code> file is shown below. If you had selected another theme, you would see the name there.</p> <pre><code>{\n  \"theme\": \"Default\",\n  \"selectedAuthType\": \"oauth-personal\"\n}\n</code></pre>"},{"location":"4_cli_build_in/","title":"Gemini CLI - In-built Tools","text":""},{"location":"5_cli_mcp/","title":"Configuring Model Context Protocol (MCP) Servers","text":""},{"location":"5_cli_mcp/#what-is-mcp","title":"What is MCP ?","text":"<p>MCP is an open standard that solves a fundamental challenge in AI system integration: the fragmentation problem.  Without MCP, every AI application needs custom integrations with every tool it wants to access. MCP replaces this chaos with a single, standardized protocol built on JSON-RPC 2.0. </p> <p></p> <p>An MCP server is an application that exposes tools and resources to the Gemini CLI through the Model Context Protocol, allowing it to interact with external systems and data sources. MCP servers act as a bridge between the Gemini model and your local environment or other services like APIs.</p> <p></p> <p>An MCP server enables the Gemini CLI to discover and execute tools thereby extending Gemini CLI's capabilities to perform actions beyond its built-in features, such as interacting with databases, APIs, custom scripts, or specialized workflows.</p> <p>Gemini CLI supports configuring MCP Servers for discovering and using custom tools. If you have Gemini CLI launched, you can check up on the MCP servers configured via the <code>/mcp</code> command.</p>"},{"location":"5_cli_mcp/#gemini-cli-mcp-config","title":"Gemini CLI MCP config","text":"<p>If you have not configured any MCP servers, it will launch Gemini CLI's MCP Server documentation.</p> <p>You can configure MCP servers at the global level in the <code>~/.gemini/settings.json</code> file or in your project's root directory. Create or open the <code>.gemini/settings.json</code> file. Within the file, you will need to add the <code>mcpServers</code> configuration block, as shown below:</p> <pre><code>\"mcpServers\": {\n  \"server_name_1\": {},\n  \"server_name_2\": {},\n  \"server_name_n\": {}\n}\n</code></pre> <p>Each server configuration supports the following properties (Reference documentation):</p> <p>Required (one of the following)</p> <ul> <li><code>command</code> (string): Path to the executable for Stdio transport</li> <li><code>url</code> (string): SSE endpoint URL (e.g., \"http://localhost:8080/sse\")</li> <li><code>httpUrl</code> (string): HTTP streaming endpoint URL</li> </ul> <p>Optional</p> <ul> <li><code>args</code> (string[]): Command-line arguments for Stdio transport</li> <li><code>headers</code> (object): Custom HTTP headers when using <code>url</code> or <code>httpUrl</code></li> <li><code>env</code> (object): Environment variables for the server process. Values can reference environment variables using <code>$VAR_NAME</code> or <code>${VAR_NAME}</code> syntax</li> <li><code>cwd</code> (string): Working directory for Stdio transport</li> <li><code>timeout</code> (number): Request timeout in milliseconds (default: 600,000ms = 10 minutes)</li> <li><code>trust</code> (boolean): When <code>true</code>, bypasses all tool call confirmations for this server (default: <code>false</code>)</li> <li><code>includeTools</code> (string[]): List of tool names to include from this MCP server. When specified, only the tools listed here will be available from this server (whitelist behavior). If not specified, all tools from the server are enabled by default.</li> <li><code>excludeTools</code> (string[]): List of tool names to exclude from this MCP server. Tools listed here will not be available to the model, even if they are exposed by the server.     Note: <code>excludeTools</code> takes precedence over <code>includeTools</code> - if a tool is in both lists, it will be excluded.</li> </ul> <p>Let us go ahead and configure one of the key MCP servers that you might need if you are working with JIRA. You would probably have a question: if you already have JIRA and other related tools set up on your system, would you still need the JIRA MCP Server?</p> <p>Gemini CLI will invoke the git tools that you have on your system and you may also instruct the Gemini CLI to use that. So do keep in mind that Gemini CLI will help in translating your natural language queries to equivalent tools that you may have on your system and it may require that you explicitly state that in your prompt.</p>"},{"location":"5_cli_mcp/#task-1-configure-jira-mcp-server","title":"Task 1 Configure JIRA MCP Server","text":"<p>The JIRA official MCP Server provides sufficient documentation on the tools that it exposes along with how to configure the same. You can pick your choice in terms of running it locally or remotely, since Gemini CLI supports remote MCP Servers too.</p> <p>This Lab demonstrates the Remote MCP Server option in JIRA. </p> <p>Once you have that, you will need to add the MCP Server object in the <code>settings.json</code> file. The complete <code>settings.json</code> file on system is shown below. You might have additional settings, but the <code>mcpServers</code> object should be as given below:</p> <p>Step 1: Crete <code>.gemini</code> folder</p> <pre><code>cd ~/gemini-cli-projects\nmkdir .gemini\ncd .gemini\n</code></pre> <p>Step 2: Paste the <code>settings.json</code> in you project level <code>gemini</code> folder</p> <pre><code>cat &lt;&lt;EOF &gt; settings.json\n{\n  \"theme\": \"Default\",\n  \"ideMode\": true,\n  \"selectedAuthType\": \"oauth-personal\",\n  \"mcpServers\": {\n       \"atlassian\": {\n           \"command\": \"npx\",\n           \"args\": [\n               \"-y\",\n               \"mcp-remote\",\n               \"https://mcp.atlassian.com/v1/sse\"\n           ]\n       }\n  }\n}\nEOF\n</code></pre> <p>Step 3: You can either start Gemini CLI again or do a <code>/mcp refresh</code> command, once you have updated the <code>settings.json</code> with the Github MCP Server configuration.</p> <pre><code>/mcp refresh\n</code></pre> <p>Step 4: Authenticate to JIRA and Confluence with you user</p> <p></p> <p>Result</p> <p>You've been granted access to use JIRA MCP servers and now gemini CLI will be able to communicate with JIRA.</p> <p>Step 5: List configured JIRA MCP servers:</p> <pre><code>/mcp list\n</code></pre> <p>Discuss</p> <p>Review what tools JIRA MCP server has (e.g create issue, fetch issue, Add comments to JIRA)</p> <p></p> <p>Step 6: Interact with JIRA from you CLI!</p> <pre><code>- \"List all the issues in the boutique project.\"\n- \"Create a new issue SCRUM-2 in the boutique project titled 'UI update' and describe it as 'Implement the new feature as for UI .'\"\n- \"Add a comment to SCRUM-2 saying 'This is a comment'\"\n- \"Show me the details for issue SCRUM-2.\"\n- \"Assign issue SCRUM-2 to Archy k.\"\n- \"Move SCRUM-2 to 'In Progress'.\"\n</code></pre>"},{"location":"5_cli_mcp/#more-mcp-servers","title":"More MCP Servers","text":"<p>Here is an additional list of MCP servers that you might be interested in:</p> <ul> <li>Firebase MCP Server</li> <li>Google Gen AI Media Services (Imagen, Veo, Lyria)</li> <li>See Appendix A:   MCP Toolbox for Databases (work with Firestore, BigQuery, Google Cloud databases)</li> <li>Google Workspace MCP Server (work with Docs, Sheets, Calendar, Gmail)</li> </ul> <p>The instructions for setting up the above MCP servers are published in this blog post.</p>"},{"location":"5_cli_mcp/#jira-prompts-cheatsheet","title":"JIRA Prompts Cheatsheet","text":"<p>Once the JIRA MCP Server is configured, you can interact with JIRA using natural language prompts. Here are some examples of what you can ask Gemini CLI to do:</p>"},{"location":"5_cli_mcp/#general","title":"General","text":"<ul> <li>\"Show me what I can do with JIRA\"</li> <li>\"How do I work with issues in JIRA?\"</li> <li>\"Tell me about JIRA projects\"</li> </ul>"},{"location":"5_cli_mcp/#issue-management","title":"Issue Management","text":"<ul> <li>\"Create a new issue SCRUM-2 in the boutique project titled 'UI update' and describe it as 'Implement the new feature as for UI .'\"</li> <li>\"List all the issues in the boutique project.\"</li> <li>\"Add a comment to SCRUM-2 saying 'This is a comment'\"</li> <li>\"Show me the details for issue SCRUM-2.\"</li> <li>\"Assign issue SCRUM-2 to Archy k.\"</li> <li>\"Move SCRUM-2 to 'In Progress'.\"</li> </ul>"},{"location":"5_cli_mcp/#advanced-workflows","title":"Advanced Workflows","text":"<ul> <li>\"Create a sub-task for boutique-123 with the title 'Sub-task for feature'.\"</li> <li>\"Find all 'In Progress' issues in the boutique project, ordered by creation date.\"</li> <li>\"Log 2 hours and 30 minutes of work on issue boutique-123.\"</li> <li>\"Link issue boutique-123 as 'blocks' issue boutique-456.\"</li> </ul>"},{"location":"5_cli_mcp/#sprint-manaboutiqueent","title":"Sprint Manaboutiqueent","text":"<ul> <li>\"Create a new sprint named 'New Sprint' on board 1.\"</li> <li>\"Add issue boutique-123 to the current sprint.\"</li> <li>\"Start sprint 42 and set its duration to two weeks.\"</li> <li>\"Show me all the issues in the active sprint.\"</li> </ul>"},{"location":"6_cli_custom_commands/","title":"\ud83d\udee0\ufe0f Custom / Commands","text":""},{"location":"6_cli_custom_commands/#why-custom-slash-commands","title":"Why custom / slash commands?","text":"<p>If you noticed your usage of Gemini CLI so far, you would have noticed that either we were just giving the prompt and expecting Gemini CLI to execute it, sometimes with results that are not as per our expectations. In some prompts, you were a bit specific in terms of what to do and have included those instructions in the prompt.</p> <p>This may work fine as per what you are instructing Gemini CLI to do and the results that you get. But in many cases, you want to ensure that it follows rules. These rules could be specific programming languages or frameworks to use. It could also be specific tools. It could be coding styles. It's not just about generation but you might also want Gemini CLI to strictly be in what is called a \"planning\" mode and would like it to just present a plan and not generate any code or modify files on the system.</p> <p>Enter <code>Custom slash commands</code> in Gemini CLI which allow you to save and reuse your most frequently used prompts as personal shortcuts. You can create commands that are specific to a single project or available globally across all your projects.</p>"},{"location":"6_cli_custom_commands/#how-slash-commands-work","title":"How / slash commands work?","text":"<p>Custom commands allow you to create powerful, reusable prompts. They are defined in TOML files and stored in a <code>commands</code> directory.</p> <ul> <li>Global commands: <code>~/.gemini/commands/</code></li> <li>Project-specific commands: <code>&lt;project&gt;/.gemini/commands/</code></li> </ul> <p>To create a custom command, you need to create a <code>.toml</code> file in one of these directories.</p> <p>For example, to create a <code>/plan</code> command, you would create a <code>plan.toml</code> file. Each TOML file must have a <code>description</code> and a <code>prompt</code>.</p> <p>Here is an example of a <code>plan.toml</code> file:</p> <pre><code>description=\"Investigates and creates a strategic plan to accomplish a task.\"\n\nprompt = \"\"\"\nYour primary role is that of a strategist, not an implementer.\nYour task is to stop, think deeply, and devise a comprehensive strategic plan to accomplish the following goal: {{args}}\nYou MUST NOT write, modify, or execute any code. Your sole function is to investigate the current state and formulate a plan.\nUse your available \"read\" and \"search\" tools to research and analyze the codebase. Gather all necessary context before presenting your strategy.\nPresent your strategic plan in markdown. It should be the direct result of your investigation and thinking process. Structure your response with the following sections:\n\n1. **Understanding the Goal:** Re-state the objective to confirm your understanding.\n2. **Investigation &amp; Analysis:** Describe the investigative steps you would take. What files would you need to read? What would you search for? What critical questions need to be answered before any work begins?\n3. **Proposed Strategic Approach:** Outline the high-level strategy. Break the approach down into logical phases and describe the work that should happen in each.\n4. **Verification Strategy:** Explain how the success of this plan would be measured. What should be tested to ensure the goal is met without introducing regressions?\n5. **Anticipated Challenges &amp; Considerations:** Based on your analysis, what potential risks, dependencies, or trade-offs do you foresee?\n\nYour final output should be ONLY this strategic plan.\n\"\"\"\n</code></pre> <p>Note</p> <p>The <code>{{args}}</code> placeholder in the prompt will be replaced with any text the user types after the command name. After creating the <code>.toml</code> file and restarting Gemini CLI, the new slash command will be available.</p>"},{"location":"6_cli_custom_commands/#task-1-configure-plan-custom-command","title":"Task 1 Configure /plan custom command","text":"<p>We've prepared set of custom / commands for you usage here:</p> <p>Review the github repo and explore how <code>.toml</code> files are looking like. Below is the summary table how to use those commands.</p> Command Description explain Explain mode. Analyzes the codebase to answer questions and provide insights explain:interactive Interactive Explore mode. Interactively explains the codebase through guided discovery. plan:new Plan mode. Generates a plan for a feature based on a description plan:refine Refinement mode. Refines an existing plan based on user feedback. plan:impl Implementation mode. Implements a plan for a feature based on a description deploy:project Deploys the project to a target environment <p>Step 1: Install Custom Commands on Laptop</p> <p>You can install these configurations globally for all projects or locally for a single project.</p> <p>Important</p> <p>Choose only one of the methods below. Global or Project-Specific</p> <p>Global Installation:</p> <p>Copy the <code>commands</code> folder from the repo to the root folder of your machine under <code>~/.gemini/commands/</code></p> <pre><code>git clone --depth 1 https://github.com/GoogleCloudPlatform/serverless-production-readiness-java-gcp.git ~/.gemini-tmp &amp;&amp; \nrsync -av ~/.gemini-tmp/genai/gemini-cli-extensions/commands/ ~/.gemini/commands &amp;&amp; rm -rf ~/.gemini-tmp\n</code></pre> <p>Project-Specific Installation:</p> <p>Copy the <code>commands</code> folder from the repo to the <code>.gemini/commands/</code> folder of your project.</p> <pre><code>git clone --depth 1 https://github.com/GoogleCloudPlatform/serverless-production-readiness-java-gcp.git .gemini-tmp &amp;&amp; \nrsync -av .gemini-tmp/genai/gemini-cli-extensions/commands/ ./.gemini/commands &amp;&amp; rm -rf .gemini-tmp\n</code></pre> <p>Step 2: Check if new commands has been installed</p> <pre><code>/help\n</code></pre> <p>Output: <pre><code>Commands:                                                                                                                                                \u2502\n\u2502  /about - show version info\n....                                                        \n\u2502  /explain - Explain mode. Analyzes the codebase to answer questions and provide insights.                                                                \u2502\n\u2502  /plan:refine - Refinement mode. Refines an existing plan based on user feedback.                                                                        \u2502\n\u2502  /plan:new - Plan mode. Generates a plan for a feature based on a description                                                                            \u2502\n\u2502  /plan:impl - Implementation mode. Implements a plan for a feature based on a description                                                                \u2502\n\u2502  /generate:gemini_md - Analyzes the current project directory recursively and generate a comprehensive GEMINI.md file                                    \u2502\n\u2502  /explain:interactive - Interactive Explore mode. Interactively explains the codebase through guided discovery.                                          \u2502\n\u2502  /deploy:project - Deploys the project to a target environment.                                                                                          \u2502\n\u2502  ! - shell command                                           \n</code></pre></p> <p>Result</p> <p>New Custom /plan, /explain, and /deploy commands has been installed</p>"},{"location":"6_cli_custom_commands/#task-2-create-plan-and-implement-plan-for-new-application","title":"Task 2 Create /plan and implement plan for new application","text":"<p>Step 1 Prompt to plan a new app</p> <pre><code>/plan:new I would like to create a Python Flask Application that shows me a list of live scores of cricket matches. \nThere is a RSS Feed for this that is available over here: `https://static.cricinfo.com/rss/livescores.xml`.  \n</code></pre> <p>Alternatively</p> <p>You can create a JIRA task with above information and use JIRA MCP integration to fetch a task description</p> <p>Step 2 Review a generated plan</p> <p></p> <p>** Result**: Generated plan stored in <code>plans</code> forlder  in <code>md</code> format.</p> <p>Important</p> <p>Review and modify plan until it is required</p> <p>Step 3 Implement code for new app according to plan and deploy it</p> <pre><code>/plan:impl I would like to implement a plan and make sure the app created in a new folder \n</code></pre> <p></p> <p>Result</p> <p>The cricket scores app is created and has been launched for local testing.</p> <p></p> <p>Note</p> <p>The app looks more advanced then we created one during vibecoding test.</p>"},{"location":"6_cli_custom_commands/#task-3-create-a-plan-and-implement-new-features-in-your-existing-codebase","title":"Task 3 Create a plan and implement new features in your existing codebase.","text":"<p>Congratulations</p> <p>It is now time to put things in practice. Will let you work with you day to day applications using JIRA MCP and Custom / commands to build you apps!</p>"},{"location":"6_cli_custom_commands/#conclusion-your-workflow-your-rules","title":"Conclusion: Your Workflow, Your Rules","text":"<p>In this Lab, we have learnt about Custom Commands in Gemini CLI, which are simple shortcuts, organized into namespaces and which can run powerful and personalized tasks for us.</p>"},{"location":"7_cli_coding_style/","title":"(EXTRA) Gemini CLI - Coding styles with Gemini.md","text":"<p>The <code>GEMINI.md</code> file is a crucial component for tailoring the instructional context, or \"memory,\" provided to the Gemini model.</p> <p>This file enables you to supply:</p> <ul> <li>Pproject-specific instructions</li> <li>Coding style guidelines</li> <li>Any relevant background information to the AI</li> </ul> <p>Resulting in responses that are more accurate and customized to your needs.</p> <p>The <code>GEMINI.md</code> file is in markdown format and is loaded hierarchically from multiple locations. The loading order is as follows:</p> <ol> <li>Global Context: <code>~/.gemini/GEMINI.md</code> (for instructions that apply to all your projects).</li> <li>Project/Ancestor Context: The CLI searches from your current directory up to the project root for <code>GEMINI.md</code> files.</li> <li>Sub-directory Context: The CLI also scans subdirectories for <code>GEMINI.md</code> files, allowing for component-specific instructions.</li> </ol> <p>You can use the <code>/memory show</code> command to see the final combined context being sent to the model.[1][2]</p> <p>Here is an example of a <code>GEMINI.md</code> file:</p> <pre><code># Project: My Awesome TypeScript Library\n\n## General Instructions:\n\n- When generating new TypeScript code, please follow the existing coding style.\n- Ensure all new functions and classes have JSDoc comments.\n- Prefer functional programming paradigms where appropriate.\n- All code should be compatible with TypeScript 5.0 and Node.js 20+.\n\n## Coding Style:\n\n- Use 2 spaces for indentation.\n- Interface names should be prefixed with `I` (e.g., `IUserService`).\n- Private class members should be prefixed with an underscore (`_`).\n- Always use strict equality (`===` and `!==`).\n\n## Specific Component: `src/api/client.ts`\n\n- This file handles all outbound API requests.\n- When adding new API call functions, ensure they include robust error handling and logging.\n- Use the existing `fetchWithRetry` utility for all GET requests.\n\n## Regarding Dependencies:\n\n- Avoid introducing new external dependencies unless absolutely necessary.\n- If a new dependency is required, please state the reason.\n</code></pre> <p>You can also add to the <code>GEMINI.md</code> file during your interactive session with Gemini CLI using the <code>/memory add &lt;some instruction/rule&gt;</code> command.</p>"},{"location":"8_cli_mcp_bq/","title":"MCP Toolbox for Databases","text":"<p>If you are working with Databases, then I suggest that you look at Google project MCP Toolbox for Databases. It is an open source MCP server for databases and supports a wide variety of databases, as shown below from their official documentation:</p> <p></p> <p>The MCP Toolbox for Databases provides executables depending on your operating system/architecture and you need to configure and run it locally. The database could be local or even remote (for e.g. a Cloud SQL instance running in Google Cloud).</p> <p>Let's first download the MCP Toolbox for Databases. Before we download create a folder on your machine that will host the MCP Toolbox binary.</p> <pre><code>mkdir mcp-toolbox\ncd mcp-toolbox\n</code></pre> <p>Check out the releases page for your Operation System and Architecture and download the correct binary. I am using a Mac machine with the ARM chip, so here is the script that I am using for the latest version of Toolbox:</p> <pre><code>export VERSION=0.14.0\ncurl -O https://storage.googleapis.com/genai-toolbox/v$VERSION/darwin/arm64/toolbox\nchmod +x toolbox\n</code></pre> <p>Just keep the entire path to the toolbox ready, since we will need that when we configure this MCP Server in the Gemini CLI <code>settings.json</code> file. On my machine, I setup the toolbox in the <code>/Users/romin/mcp-toolbox</code> folder. So the entire path to the Toolbox binary is:</p> <p><code>/Users/romin/mcp-toolbox/toolbox</code></p> <p>You can refer to the official documentation on how to configure the various databases / data sources that you would the Toolbox to connect to. But one of the quickest ways to get started is via the <code>--preview</code> option for some of the databases that are supported. For e.g. the <code>--preview</code> mode is supported for Google Cloud BigQuery database, so let's try that out for my Google Cloud Project.</p> <p>The MCP server entry that needs to go into the <code>settings.json</code> file in Gemini CLI is shown below:</p> <pre><code>\"BigQueryServer\": {\n  \"command\": \"/Users/romin/mcp-toolbox/toolbox\",\n  \"args\": [\"--prebuilt\", \"bigquery\", \"--stdio\"],\n  \"env\": {\n    \"BIGQUERY_PROJECT\": \"YOUR_GOOGLE_CLOUD_PROJECT_ID\"\n  }\n}\n</code></pre> <p>Notice that the command parameter has the full path to the toolbox binary. When we start up Gemini CLI, we can see that additional MCP tools are available now:</p> <p>Let's query to see what datasets are available in my project. A sample run is shown below:</p> <p></p> <p></p> <p>If you want to customize the tools i.e. the queries and the different options that are allowed, you will need to create a file named <code>tools.yaml</code> , in which all the data source configuration, toolsets, tools and queries will be configured. You will need to provide as a command line parameter while starting up the toolbox binary via the MCP Server option. Let's configure one of the datasource to understand that better.</p> <p>Here is a configuration for the <code>tools.yaml</code> file for connecting to a public BigQuery dataset in my Google Cloud Project.</p> <pre><code>sources:\n  my-bq-source:\n    kind: bigquery\n    project: YOUR_PROJECT_ID\ntools:\n  search_release_notes_bq:\n    kind: bigquery-sql\n    source: my-bq-source\n    statement: |\n      SELECT\n        product_name,description,published_at\n      FROM\n        `bigquery-public-data`.`google_cloud_release_notes`.`release_notes`\n      WHERE\n        DATE(published_at) &gt;= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)\n      GROUP BY product_name,description,published_at\n      ORDER BY published_at DESC\n    description: |\n      Use this tool to get information on Google Cloud Release Notes.\ntoolsets:\n  my_bq_toolset:\n    - search_release_notes_bq\n</code></pre> <p>To configure this toolset/tool in Gemini CLI, we need to provide the command to configure and start the MCP Toolbox for Databases executable and provide the <code>tools.yaml</code> as shown above:</p> <pre><code>\"BigQueryServer\": {\n  \"command\": \"/Users/romin/mcp-toolbox/toolbox\",\n  \"args\": [\"--tools-file\", \"/Users/romin/mcp-toolbox/tools.yaml\", \"--stdio\"],\n  \"env\": {\n    \"BIGQUERY_PROJECT\": \"YOUR_GOOGLE_CLOUD_PROJECT_ID\"\n  }\n}\n</code></pre> <p>Do check out the configuration documentation page for MCP Toolbox for Databases for more information.</p>"},{"location":"cli_par2/","title":"Gemini CLI Tutorial Series \u2014 Part 2: Gemini CLI Command line parameters","text":"<p>By Romin Irani on Jun 30, 2025</p> <p>Welcome to Part 2 of the Gemini CLI Tutorial series. In this part, we are going to look at a few configuration options while launching Gemini CLI at the command line. This tutorial will also help us understand various other features of Gemini CLI like checkpointing, getting debug information, a look at how Gemini CLI sets up the context and more.</p> <p>This is by no means an exhaustive exploration of each and every flag or command, but it gets our foundation in place to understand what goes on behind the tool and to use a few configuration options as we need.</p>"},{"location":"cli_par2/#gemini-cli-tutorial-series","title":"Gemini CLI Tutorial Series:","text":"<ul> <li>Part 1 : Installation and Getting Started</li> <li>Part 2 : Gemini CLI Command line options (this post)</li> <li>Part 3 : Configuration settings via settings.json and .env files</li> <li>Part 4 : Built-in Tools</li> <li>Part 5: Using Github MCP Server</li> <li>Part 6: More MCP Servers : Firebase, Google Workspace, Google Gen AI Media Services and MCP Toolbox for Databases</li> <li>Part 7: Custom slash commands</li> <li>Part 8: Building your own MCP Server</li> <li>Part 9: Understanding Context, Memory and Conversational Branching</li> <li>Part 10: Gemini CLI and VS Code Integration</li> <li>Part 11: Gemini CLI Extensions</li> <li>Part 12: Gemini CLI GitHub Actions</li> <li>\u27a1\ufe0f Codelab : Hands-on Gemini CLI</li> </ul>"},{"location":"cli_par2/#blog-post-updates","title":"Blog post updates:","text":"<p>September 10, 2025 *   Validated the commands, updated screenshots for Gemini CLI v0.4.0. *   Removed the section of Telemetry from this part. It will be republished as a separate article. *   Expanded the Checkpointing coverage to include the files where checkpointing data is stored.</p> <p>A gentle reminder that Gemini CLI with its open-source nature, releases versions frequently. So if you already have Gemini CLI setup, do remember to upgrade it. Here's the command:</p> <pre><code>npm upgrade -g @google/gemini-cli\n</code></pre> <p>In the first part, we simply launched Gemini CLI via the <code>gemini</code> command at the terminal, but there are several options that we can tweak. To understand the range of options, simply give the following command <code>gemini --help</code> to see the list of options:</p> <p>Ready for some fun?</p> <p>Let's have a bit of fun first. Check out the <code>-y</code> option, which is the YOLO mode. Ideally you may not want to run Gemini CLI in this fashion, since it will automatically accept all actions. Actions like we saw in the first part, could be things like writing to files, etc \u2014 which you might want to acknowledge and permit. But this is not the fun part. The fun part is how the Gemini CLI Engineers have provided a super useful link in the above help instructions for <code>--yolo</code> mode. It has a URL, a Youtube video in fact, for you to see the details on this wonderful feature. Please go to that URL and come back with your responses to that :-)</p> <p>I hope this useful URL continues to be there in future releases too!</p>"},{"location":"cli_par2/#which-version-of-gemini-cli-are-you-running","title":"Which version of Gemini CLI are you running?","text":"<p>Let's get back to the options. The <code>-v</code> or <code>--version</code> is straightforward and we did check that out in the first part. Currently, at the time of writing, here is my Gemini CLI version.</p> <pre><code>gemini -v\n0.4.0\n</code></pre> <p>So, if you are an older version, go ahead and do that upgrade, the instruction is repeated below:</p> <pre><code>npm upgrade -g @google/gemini-cli\n</code></pre>"},{"location":"cli_par2/#specifying-a-gemini-model-for-the-gemini-cli-to-use","title":"Specifying a Gemini model for the Gemini CLI to use","text":"<p>Currently, I know of 2 models that one can specify to the Gemini CLI while starting up: <code>gemini-pro-2.5</code> and <code>gemini-flash-2.5</code> . This is specified via the <code>-m</code> or <code>--model</code> parameter as shown below:</p> <pre><code>gemini -m \"gemini-flash-2.5\"\n</code></pre> <p>The above command starts up the Gemini CLI and if all goes well you will notice the specific model listed in the status bar at the bottom.</p> <p>While this is good, here is what I have seen happen and its understandable in the free tier. If you are using the free tier of Gemini CLI with your personal Google account, you will find that even if you choose the <code>gemini-2.5-pro</code> model, the CLI adjusts to the <code>gemini-2.5-flash</code> model due to quota issues. So be prepared for that. This can be addressed to the best of my knowledge by switching over to using your own Gemini API Key.</p>"},{"location":"cli_par2/#one-prompt-at-a-time","title":"One prompt at a time","text":"<p>How about executing Gemini CLI in a way that it does not bring up the terminal interface. Instead, it just takes your prompt, executes it and gives back the result. This is known as the non-interactive mode.</p> <p>This can be very useful in scenarios where you want to integrate Gemini CLI in automated pipelines or schedule something at a specific time, without the need for any human interface. This is flexibility of the Gemini CLI interface where its not just about running something inside of a terminal but can be integrated in various other scenarios and execution modes too.</p> <p>So coming back to what we wanted to try? In case you have a need to simply prompt the Gemini CLI and do not require the terminal interface to come up, try the <code>-p</code> or the <code>--prompt</code> option, as shown below:</p> <pre><code>gemini -p \"What is the gcloud command to deploy an application to Google Cloud Run\"\n</code></pre> <p>or</p> <pre><code>gemini -p \"What is the command line syntax for doing a GET call to myhost.com via curl\"\n</code></pre> <p>This might be a good way to get some quick answers but do keep in mind that there is be no scope to continue the conversation with follow up questions.</p>"},{"location":"cli_par2/#positional-prompt-instead-of-prompt-parameter","title":"Positional Prompt instead of prompt parameter","text":"<p>From what I understand at the time of writing (v0.4.0), this is being deprecated in favour of positional prompt. This means that we can simply give the prompt words after <code>gemini</code>.</p> <p>For example, just try this:</p> <pre><code>gemini \"What is the gcloud command to deploy an application to Google Cloud Run\"\n</code></pre> <p>or</p> <pre><code>gemini \"What is the command line syntax for doing a GET call to myhost.com via curl\"\n</code></pre>"},{"location":"cli_par2/#d-stands-for-debug","title":"d stands for debug","text":"<p>You might not need this option ideally but in case you are reporting an issue or even to understand a bit about what is going on, its helpful to see what happens when we use the debug flag i.e. <code>-d</code> or <code>--debug</code></p> <p>First, let me show you the <code>-d</code> option while a single prompt (using <code>-p</code> ) at the command line. Before I go into launching the Gemini CLI and its output, let me describe the current folder that I am in and its contents. This is important for you too.</p> <p>I am launching Gemini CLI from <code>/Users/romin/gemini-cli-projects</code> folder. This folders has several folders, which contain apps that I have generated using Gemini CLI itself.</p> <p>So, I go ahead and give the following command below.</p> <pre><code>gemini -d -p \"What is the Linux command to move files recursively from one folder to another. Give me an example or two\"\n</code></pre> <p>This results in the following output. I have truncated some of the file listings, so that we keep the focus on what is going on. A better understanding of this output is critical so that you can understand the hierarchy in which Gemini CLI looks at certain files to load in order to setup the context (hint : <code>GEMINI.md</code> )</p> <p>You will notice that the Gemini CLI starts building up a context and tries to find the <code>GEMINI.md</code> file. It keeps looking recursively till it has reached the root. At the moment, I do not have any <code>GEMINI.md</code> file and hence it could not find that. But if it did, it would have found all the <code>GEMINI.md</code> files and concatenated them to create, as the documentation states \u201cinstructional context (also referred to as \u201cmemory\u201d) provided to the Gemini model. This powerful feature allows you to give project-specific instructions, coding style guides, or any relevant background information to the AI, making its responses more tailored and accurate to your needs\u201d.</p> <p>We will get to the <code>GEMINI.md</code> file a bit later, but here are a couple of things that I believe you should understand if using the Gemini CLI tool.</p>"},{"location":"cli_par2/#what-is-geminimd-and-why-do-i-need-it","title":"What is GEMINI.md and why do I need it?","text":"<p>Let's relook at the high level information around <code>GEMINI.md</code> as mentioned in the documentation over here. Its actually quite well written and hence I do not wish to recreate it in some other way. I have highlighted key things in bold.</p> <p>While not strictly configuration for the CLI's behavior, context files (defaulting to <code>GEMINI.md</code> but configurable via the <code>context.fileName</code> setting) are crucial for configuring the instructional context (also referred to as \"memory\") provided to the Gemini model. This powerful feature allows you to give project-specific instructions, coding style guides, or any relevant background information to the AI, making its responses more tailored and accurate to your needs. The CLI includes UI elements, such as an indicator in the footer showing the number of loaded context files, to keep you informed about the active context.</p> <p>Purpose: These Markdown files contain instructions, guidelines, or context that you want the Gemini model to be aware of during your interactions. The system is designed to manage this instructional context hierarchically.</p> <p>There are a lot of articles that cover how best to write <code>GEMINI.md</code> files and it will continue to be something that keeps getting covered. For the moment, it is sufficient to understand that this is one of the key mechanisms by which we can instruct the Gemini CLI to follow our rules/recommendations in how code is generated, any versions, how dependencies should be managed, coding guidelines, etc. You get the gist of where this is going.</p> <p>I reproduce again, a part of the documentation, that shows a sample <code>GEMINI.md</code> file for a Typescript project. Even if you are not a Typescript person, this is a good template to take, customize it for your preferences, language, frameworks and more.</p> <p>If you have been working with LLMs for a while, you will know by now that these instructions are provided in good faith to the model and the exact results might not be what you want. It may require significant tuning of <code>GEMINI.md</code> , some additional parameters and more. Hence we should ideally be looking out for articles where Gemini CLI users will share what's worked best for them and learn together.</p> <p>Having said that, you can use Gemini CLI without a <code>GEMINI.md</code> file too. But you will quickly notice that eventually it would be good to have these files, which may provide overall global instructions for anything you do with Gemini CLI and then have project or task specific <code>GEMINI.md</code> files too. That's just a pattern that I believe will emerge. But don't fret too much on that at the moment. I just mentioned multiple <code>GEMINI.md</code> files and that brings us to the next question.</p>"},{"location":"cli_par2/#where-should-my-geminimd-files-reside","title":"Where should my GEMINI.md files reside?","text":"<p>We have quickly established the fact that we may want multiple <code>GEMINI.md</code> files and earlier, we have highlighted that Gemini CLI will find all of them and concatenate them together into a single instructional set that it will use. Content from files lower in this list (more specific) typically overrides or supplements content from files higher up (more general).</p> <p>So whats, the order in which the <code>GEMINI.md</code> will be searched? I summarize it precisely for you, from the documentation:</p> <p>Context is loaded from three main levels, moving from broadest to most specific:</p> <ol> <li>Global Context File: A <code>GEMINI.md</code> file in your home directory for rules across all projects. This is a special directory named <code>~/.gemini</code>.</li> <li>Project and Ancestors Context Files: Files in the project's root directory for project-wide rules.</li> <li>Local Context Files: Files in sub-directories for highly specific instructions about a particular module or component.</li> </ol> <p>That brings us back to the output that we got from the <code>-d</code> (debug) command and do note that while I did not have any <code>GEMINI.md</code> , you can see how it went about searching for it.</p> <p>Let's see what happens when we launch the Gemini CLI without the <code>-p</code> option but with just the <code>-d</code> option. We launch it as follows:</p> <pre><code>gemini -d\n</code></pre> <p>This brings up the CLI as shown below:</p> <p>Notice a few additional things in the debug mode. It displayed what the authentication method was ( <code>vertex-ai</code> ), which is what I have used on my system but in your case, if you are using your Google personal account, it would be <code>oauth-personal</code> . There is the <code>--debug</code> flag shown below. Additionally you also see now the memory usage (for e.g. 198 MB).</p> <p>At this point, if I do a <code>/memory show</code> command, I can see the current context, what files were loaded, etc. This is very useful to understand if the context was loaded correctly. While I do not have a <code>GEMINI.md</code> file at the moment, due to which there is nothing available as per the output below.</p> <p>If you happen to have a <code>GEMINI.md</code> file, modify it outside, you can always refresh it via the <code>/memory refresh</code> command. Try that command right now and see the similar debug statements that come up.</p> <p>Whew ! That was a lot of discussion just for <code>debug</code> . But do keep this option handy. In case you are reporting any specific issues that you find, it might be handy to turn on the debug mode and see what is going on.</p>"},{"location":"cli_par2/#lets-have-a-checkpoint","title":"Let's have a checkpoint","text":"<p>This is an interesting but an essential feature. Imagine you are going on with the Gemini CLI and it has used tools to write to file, etc. But something goes wrong and you would like to get back to the previous good state. That's the checkpointing feature, which as the documentation states \u201cautomatically saves a snapshot of your project's state before any file modifications are made by AI-powered tools. This allows you to safely experiment with and apply code changes, knowing you can instantly revert back to the state before the tool was run. \u201d</p> <p>By default, when you launch the Gemini CLI, checkpointing is not enabled and you will need to enable it via the <code>-c</code> or <code>--checkpointing</code> flag, when you launch it.</p> <p>Let's see this feature in action. First up, I have a small Go project. It is a command line utility that takes a sample text file and it provides the number of characters, words and lines in the text file. So I am in that current folder that has a <code>main.go</code> file and a sample file to test <code>test.txt</code> . This is also a Git enabled folder and I have committed <code>main.go</code> , <code>test.txt</code> and <code>go.mod</code> .</p> <p>I launch Gemini CLI with the checkpointing and debugging enabled as given below:</p> <pre><code>gemini -c -d\n</code></pre> <p>This brings up the Gemini CLI as shown below:</p> <p>The status bar clearly shows that I am in <code>--debug</code> mode and the folder along with the Git branch are shown.</p> <p>Let's run a shell command via the <code>!</code> to just validate a few things.</p> <p>So I have run two commands, the <code>pwd</code> and the <code>ls</code> command:</p> <p>Hit <code>ESC</code> key to come out of the Shell mode.</p> <p>So we are going to start here.</p> <p>At this point, there is nothing to restore or go back to. If you need to restore back to some point, you do that via the <code>/restore</code> command. If you just use the command as is, it says that there are no restorable tool calls found. And this is fair since we have not asked Gemini to do any task, that resulted in things like creating folders, writing files, etc.</p> <p>Everything looks good. Let's use the Gemini CLI to now generate a <code>README.md</code> file for this project. I give the following command:</p> <p>This results in a generation of the file and its not yet written. It is asking me for permission to use the <code>WriteFile</code> tool.</p> <p>I go ahead and give it permission. Everything goes fine and it tells me that the file has been created.</p> <p>At this point, if I run a shell command or even use <code>@</code> to display the contents, I can get that:</p> <p>If you now run the command <code>/restore</code> , you can see that there is a checkpoint that it is generated. As per the documentation \u201cthese file names are typically composed of a timestamp, the name of the file being modified, and the name of the tool that was about to be run (e.g., <code>2025\u201309\u201310T06\u201356\u201349_549Z-README.md-write_file</code> ).</p> <p>So, when we run this, we see the following:</p> <p>You can see that it has the <code>timestamp-&lt;filename&gt;-&lt;toolname&gt;</code>.</p> <p>Before we make another change to the <code>README.md</code> file, let us see where the checkpoint data is being stored? Go to the home directory (e.g. <code>~</code> on my Mac). This will have a <code>.gemini/tmp</code> folder. Expand that to see a folder that will contain your logs, shell history and checkpoints. A sample for the above operations that we have done is shown below:</p> <p>If you click on the checkpoints file ( <code>2025\u201309\u201310T06\u201356\u201349_549Z-README.md-write_file.json</code> ) you will see the details for the interaction that has happened so far.</p> <p>Cool ! Let's make another change in the <code>README.md</code> file that we have. At the top, the title reads <code>Go Word Counter</code> . I would like this to be changed to <code>File Stats utility</code> .</p> <p>The prompt is: <code>In the @README.md file, change the title to \u201cFile Stats Utility\u201d</code></p> <p>We run the above prompt as shown below:</p> <p>It does a good job and it's asking me to apply the change, which I am ok with. It says that it has completed the task and at this point, if I go to shell mode and ask it to print out <code>README.md</code> , I can see that it has done that:</p> <p>Let's use the <code>/restore</code> command now to see if another checkpoint has been created and indeed there is one:</p> <p>So we can see that we have two checkpoints, one for the <code>README.md</code> file creation and the other for the task that replaced the title.</p> <p>At this point, if you observe the <code>.gemini/tmp/&lt;CHECKPOINT&gt;</code> specific folder as explained above, you will see that there are two checkpoints now:</p> <p>Use the <code>/restore</code> command and you can now scroll through any of the checkpoints and go back to that state. So I can just go to say the second one , which only replaced the title and get it back to the original title i.e. <code>Go Word Counter</code> .</p> <p>I do that and this is important, it shows me the changes that were done at this step, which I want to revert. So when it asks me for permission, I select <code>4. No, suggest changes</code> option and I am back to my previous state.</p> <p>It will display a message that should include:</p> <p>If I view the contents of the <code>README.md</code> file, I see the following:</p> <p>Remember that Checkpointing is not available by default, so you need to provide the checkpointing flag when you start the Gemini CLI or better still, if this is a feature that you absolutely want running all the time, then you can put that in the <code>settings.json</code> file, which we shall see in the next part of this series.</p> <p>Check out the documentation for more details on Checkpointing.</p> <p>gemini-cli/docs/checkpointing.md at 770f862832dfef477705bee69bd2a84397d105a8 \u00b7\u2026</p>"},{"location":"cli_par2/#session-summary","title":"Session summary","text":"<p>If you are looking for your session summary to be persisted, which you can then look into, you can try the <code>--session-summary</code> parameter.</p> <p>Launch it as follows: <code>gemini --session-summary \"session.txt\"</code></p> <p>This will launch Gemini CLI and keep a track of the. A sample run for my project folder and a couple of interactions to get the list of files and to get an explanation for a file is shown below:</p> <pre><code>{\n  \"sessionMetrics\": {\n    \"models\": {\n      \"gemini-2.5-pro\": {\n        \"api\": {\n          \"totalRequests\": 1,\n          \"totalErrors\": 0,\n          \"totalLatencyMs\": 9119\n        },\n        \"tokens\": {\n          \"prompt\": 7068,\n          \"candidates\": 131,\n          \"total\": 7816,\n          \"cached\": 0,\n          \"thoughts\": 617,\n          \"tool\": 0\n        }\n      },\n      \"gemini-2.5-flash\": {\n        \"api\": {\n          \"totalRequests\": 1,\n          \"totalErrors\": 0,\n.\n.\n.\n</code></pre>"},{"location":"cloud_commands/","title":"Cloud Commands","text":"<p>Most people uses some flavour of Cloud hosted K8s clusters. This section is about the main K8s Cloud Providers.</p>"},{"location":"cloud_commands/#gcloud","title":"Gcloud","text":"<p>With Google, everything goes throu the <code>gcloud</code> command.</p>"},{"location":"cloud_commands/#install","title":"Install","text":"Apple Mac OSXLinuxWindows <pre><code>brew install --cask google-cloud-sdk\n</code></pre> <p>Refer to the official doc.</p> <pre><code>curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-405.0.1-linux-x86_64.tar.gz\ntar -xf google-cloud-cli-405.0.1-linux-x86.tar.gz\n./google-cloud-sdk/install.sh\n</code></pre> <p>TODO</p>"},{"location":"cloud_commands/#setup","title":"Setup","text":"<p>Once the <code>gcloud</code> command is installed, we have to init and configure it:</p> <p><pre><code># Install kubectl if you don't already have it\n# gcloud components install kubectl # Optional\n\ngcloud init\ngcloud components install gke-gcloud-auth-plugin --quiet\ngcloud auth login\n\ngcloud config set compute/region us-east1\n\ngcloud config list\n</code></pre> output<pre><code>[compute]\nregion = us-central1\nzone = us-central1-a\n[core]\naccount = prune@not-your-business.zap\ndisable_usage_reporting = False\nproject = my-dev-project\n</code></pre></p> <p>List clusters:</p> <p><pre><code>gcloud container clusters  list\n</code></pre> output<pre><code>NAME                         LOCATION       MASTER_VERSION    MASTER_IP       MACHINE_TYPE   NODE_VERSION       NUM_NODES  STATUS\nmy-dev-us-central-cluster    us-central1-a  1.24.3-gke.2100   34.70.94.2      e2-standard-2  1.23.8-gke.1900 *  136        RUNNING\n</code></pre></p> <p>Add your <code>GKE</code> clusters to your <code>kubectl</code> context (you can always find this command in the <code>Connect</code> tab in the Gcloud Web Console):</p> <pre><code>gcloud container clusters get-credentials &lt;cluster&gt; --project &lt;project&gt;\n</code></pre> <p>Note</p> <p>It is not necessary to add the <code>--project &lt;project&gt;</code> section if only one project is used and is the default. </p>"},{"location":"cloud_commands/#completion","title":"Completion","text":"<p>Add those lines to enable completion:</p> Apple Mac OSX ZSHApple Mac OSX BASHLinux ZSHLinux BASH ~/.zshrc<pre><code>source \"/usr/local/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/path.zsh.inc\"\nsource \"/usr/local/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/completion.zsh.inc\"\n</code></pre> ~/.bashrc<pre><code>source \"/usr/local/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/path.bash.inc\"\nsource \"/usr/local/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/completion.bash.inc\"\n</code></pre> ~/.zshrc<pre><code># path may vary depending on where you installed gcloud command\n$HOME/kubernetes/google-cloud-sdk/path.zsh.inc\n$HOME/kubernetes/google-cloud-sdk/completion.zsh.inc\n</code></pre> ~/.bashrc<pre><code># path may vary depending on where you installed gcloud command\n$HOME/kubernetes/google-cloud-sdk/path.bash.inc\n$HOME/kubernetes/google-cloud-sdk/completion.bash.inc\n</code></pre>"},{"location":"cloud_commands/#aws-cli","title":"AWS CLI","text":""},{"location":"cloud_commands/#install_1","title":"Install","text":"<pre><code>curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\"\nsudo installer -pkg AWSCLIV2.pkg -target /\n</code></pre>"},{"location":"cloud_commands/#setup_1","title":"Setup","text":"<p>Setup SSO ans default AWS profile. This is not mandatory but is a great helper if you're using SSO:</p> <pre><code>unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY\naws configure sso\naws sso login --profile profile_xxxxxx\nexport AWS_PROFILE=profile_xxxxxx\n</code></pre> <p>Add your <code>EKS</code> clusters to your <code>kubectl</code> context:</p> <pre><code>aws eks update-kubeconfig \\ \n    --region us-east-1    \\\n    --name &lt;cluster_name&gt; \\\n    --alias &lt;friendly_name&gt;\n</code></pre>"},{"location":"cloud_commands/#completions","title":"Completions","text":"<p>Add those lines to your shell startup script:</p> ZSHBASH ~/.zshrc<pre><code>export AWS_DEFAULT_REGION=us-east-1 # update to your preference\nexport AWS_PAGER=\"\" # prevent aws cli to auto-page = display inline\nexport BROWSER=echo # Do not open a browser, let you choose which browser to open\n# AWS CLI completions\ncomplete -C '/usr/local/bin/aws_completer' aws\n</code></pre> ~/.bashrc<pre><code>export AWS_DEFAULT_REGION=us-east-1 # update to your preference\nexport AWS_PAGER=\"\" # prevent aws cli to auto-page = display inline\nexport BROWSER=echo # Do not open a browser, let you choose which browser to open\n# AWS CLI completions\ncomplete -C '/usr/local/bin/aws_completer' aws\n</code></pre>"},{"location":"cloud_commands/#azure","title":"Azure","text":"<pre><code>TODO\n</code></pre>"},{"location":"cloud_commands/#next","title":"Next","text":"<p>Tired of typing ? Let's use a UI !</p>"},{"location":"kubectl/","title":"Kubectl install","text":""},{"location":"kubectl/#using-brew","title":"using Brew","text":"<p>this is the simplest for OsX:</p> <pre><code>brew install kubectl\n</code></pre>"},{"location":"kubectl/#installing-from-the-source-project","title":"Installing from the source project","text":"<p>This is the prefered installation for Linux or when you want full control.</p> <p>If you want to install a specific version, check the the list of official releases.</p> Apple Mac OsXLinuxWindows OsX Install<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl\"\nchmod 755 kubectl\nmv kubectl /usr/local/bin\n\n# install a specific version\ncurl -LO \"https://dl.k8s.io/release/v1.25.0/bin/darwin/amd64/kubectl\"\nchmod 755 kubectl\nmv kubectl /usr/local/bin\n\n# for Apple M1/M2 processors\ncurl -LO \"https://dl.k8s.io/release/v1.25.0/bin/darwin/arm64/kubectl\"\nchmod 755 kubectl\nmv kubectl /usr/local/bin\n</code></pre> Linux Install<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod 755 kubectl\nmv kubectl /usr/local/bin\n\n# install a specific version\ncurl -LO \"https://dl.k8s.io/release/v1.25.0/bin/linux/amd64/kubectl\"\nchmod 755 kubectl\nmv kubectl /usr/local/bin\n</code></pre> Windows Install<pre><code>curl -LO \"https://dl.k8s.io/release/v1.25.0/bin/windows/amd64/kubectl.exe\"\n</code></pre>"},{"location":"kubectl/#install-from-gcloud","title":"Install from Gcloud","text":"<p>If you already have <code>gcloud</code> command installed, you can install <code>kubectl</code>:</p> <pre><code>gcloud components install kubectl\n</code></pre>"},{"location":"kubectl/#check-kubectl-version","title":"Check Kubectl version","text":"<p>the <code>version</code> command tells which <code>client</code> and <code>server</code> versions you're using. </p> <p><pre><code>kubectl version\n</code></pre> <pre><code>Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.3\", GitCommit:\"434bfd82814af038ad94d62ebe59b133fcb50506\", GitTreeState:\"clean\", BuildDate:\"2022-10-12T10:47:25Z\", GoVersion:\"go1.19.2\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nKustomize Version: v4.5.7\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n</code></pre></p> <p>If no cluster is configured, then an error will be displayed for the <code>Server Version</code>. Else, the output will give infos of the current cluster:</p> <pre><code>Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.2\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20+\", GitVersion:\"v1.20.9-gke.1001\"}\n\nWARNING: version difference between client (1.22) and server (1.20) exceeds the supported minor version skew of +/-1\n</code></pre> <p>Note</p> <p>as returned here, it's usually recommended to use a <code>kubectl</code> version + or - one version away from the <code>server</code> version.</p>"},{"location":"kubectl/#next","title":"Next","text":"<p>Read next chapter to lean about local Kubernetes clusters</p>"},{"location":"kubectl/#ref","title":"Ref","text":"<ul> <li>gcloud</li> <li>official install doc</li> </ul>"},{"location":"mkdocs/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"mkdocs/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"mkdocs/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"mkdocs/#code-test","title":"code test","text":"<p>code highlight</p> <p>Code highlight is using a special extension</p> <p>Check how to use notes</p> code highlight 2 <p>Code highlight is using a special extension</p> <p>This is an inline command: <code>for i in * ; do echo $i ; done</code>. bash test script<pre><code>cd /tmp\nls -l\ndu -sh *\n</code></pre></p> jsonyaml <pre><code>{\n    \"kind\": \"Deployment\",\n    \"apiVersion\": \"apps/v1\",\n    \"metadata\": {\n        \"name\": \"test\",\n        \"creationTimestamp\": null,\n        \"labels\": {\n            \"app\": \"test\"\n        }\n    },\n    \"spec\": {\n        \"replicas\": 1,\n        \"selector\": {\n            \"matchLabels\": {\n                \"app\": \"test\"\n            }\n        },\n        \"template\": {\n            \"metadata\": {\n                \"creationTimestamp\": null,\n                \"labels\": {\n                    \"app\": \"test\"\n                }\n            },\n            \"spec\": {\n                \"containers\": [\n                    {\n                        \"name\": \"alpine\",\n                        \"image\": \"alpine\",\n                        \"resources\": {}\n                    }\n                ]\n            }\n        },\n        \"strategy\": {}\n    },\n    \"status\": {}\n}\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\ncreationTimestamp: null\nlabels:\n    app: test\nname: test\nspec:\nreplicas: 1\nselector:\n    matchLabels:\n    app: test\nstrategy: {}\ntemplate:\n    metadata:\n    creationTimestamp: null\n    labels:\n        app: test\n    spec:\n    containers:\n    - image: alpine\n        name: alpine\n        resources: {}\nstatus: {}\n</code></pre>"},{"location":"mkdocs/#tables","title":"Tables","text":"Method Description <code>GET</code> :material-check:     Fetch resource <code>PUT</code> :material-check-all: Update resource <code>DELETE</code> :material-close:     Delete resource"},{"location":"shell_setup/","title":"Shell Setup","text":"<p>As of now, <code>kubectl</code> is installed and can reach two working local clusters. Still, we are required to type the <code>kubectl</code> command (7 letters) quite often. Too often.</p> <p>We also have no knowledge of which cluster am I connected to ? or what is my default namespace ?.</p> <p>Let's now configure the <code>shell</code> to solve all of those problems.</p>"},{"location":"shell_setup/#im-typing-too-much-kubectl-commands","title":"I'm typing too much <code>kubectl</code> commands","text":"<p>First trick is to use <code>k</code> as an alias to <code>kubectl</code>.</p> <p>This is 6 less caracteres to type for every command !</p> zshbash ~/.zshrc<pre><code>alias k=kubectl\n</code></pre> ~/.bashrc<pre><code>alias k=kubectl\n</code></pre> <p>Open a new Shell and start using <code>k</code> command:</p> <pre><code>k get namespaces\nk get pods -A\nk get services -A\n</code></pre>"},{"location":"shell_setup/#bonus","title":"Bonus","text":"<p>Most Kubernetes resources have a short-name. You can save few keystrokes here again. </p> <p>Resources are also defined as <code>plural</code> by default (with an <code>s</code> at the end, like <code>services</code>),  but using the <code>singular</code> works the same ! It's one less keystroke again:</p> <p><pre><code>k api-resources\n</code></pre> <pre><code>NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND\npods                              po           v1                                     true         Pod\ndeployments                       deploy       apps/v1                                true         Deployment\ningresses                         ing          networking.k8s.io/v1                   true         Ingress\nservices                          svc          v1                                     true         Service\nnamespaces                        ns           v1                                     false        Namespace\n...\n</code></pre></p> <p>Let's try it:</p> <pre><code>k get ns\nk get po -A\nk get svc -A\n</code></pre>"},{"location":"shell_setup/#kubectl-arguments-are-too-long","title":"<code>kubectl</code> arguments are too long","text":"<p><code>kubectl port-forward</code> is one example of a long argument. So is <code>kubectl get deployment --context=my-cluster</code>. </p> <p><code>completion</code> is the term used in UNIX\u2122 shells when it comes to empowering the shell with better knowledge of the commands we use.</p> <p>With Completion setup, you can type <code>kubectl &lt;tab&gt;</code> or <code>k &lt;tab&gt;</code> to get help. <code>k po&lt;tab&gt;</code> will get completed to <code>k port-forward</code></p> <p>Let's add those commands to the shell config file. A restart of the shell is needed for it to take effect:</p> zshbash ~/.zshrc<pre><code>autoload -Uz compinit\ncompinit\nsource &lt;(kubectl completion zsh)\n</code></pre> ~/.bashrc<pre><code>source &lt;(kubectl completion bash)\ncomplete -o default -F __start_kubectl k\n</code></pre> <p><code>bash</code> requires the use of the <code>complete</code> command to make <code>k</code> also use completion.</p> <p>Let's try it, type <code>k &lt;tab&gt;</code> then <code>k port&lt;tab&gt;</code> then <code>k get po -n &lt;tab&gt;</code>:</p> <pre><code>k &lt;tab&gt;\nalpha          -- Commands for features in alpha\nannotate       -- Update the annotations on a resource\napi-resources  -- Print the supported API resources on the server\napi-versions   -- Print the supported API versions on the server, in the form of \"group/version\"\napply          -- Apply a configuration to a resource by file name or stdin\nattach         -- Attach to a running container\nauth           -- Inspect authorization\nautoscale      -- Auto-scale a deployment, replica set, stateful set, or replication controller\ncertificate    -- Modify certificate resources.\ncluster-info   -- Display cluster information\n...\n\nk port&lt;tab&gt;\nk port-forward\n\nk get po -n &lt;tab&gt;\ndefault             kube-node-lease     kube-public         kube-system         local-path-storage\n</code></pre>"},{"location":"shell_setup/#my-kubectl-commands-are-failing","title":"My <code>kubectl</code> commands are failing","text":"<p>Some OS, including MacOsX, comes with a really low value for the number of concurrent Open File Descriptors.</p> <p>This low limit may break some <code>kubectl</code> commands or some other tools using the Kubernetes Go client like <code>helm</code>. </p> <p>To solve that, increase your File Descriptors to at lease 2048:</p> zshbash ~/.zshrc<pre><code>ulimit -n 2048          # kubectl opens one cnx (file) per resource\n</code></pre> ~/.bashrc<pre><code>ulimit -n 2048          # kubectl opens one cnx (file) per resource\n</code></pre>"},{"location":"shell_setup/#zsh-shell","title":"ZSH shell","text":"<p>Mac OSX default shell is now <code>zsh</code> so we'll focus on ZSH now on. ZSH is also widely available in Linux and may somtimes be the default too.</p> <p>ZSH deprecation warning</p> <p>To get rid of the deprecation warning in OSX, export this variable in your shell. Add it to your <code>.zshrc</code> file for persistance:</p> ~/.zshrc<pre><code>export BASH_SILENCE_DEPRECATION_WARNING=1\n</code></pre>"},{"location":"shell_setup/#oh-my-zsh","title":"Oh My ZSH !","text":"<p>A lot of tools exist to configure and enhence any Shell to make it more productive, add colors, change the prompt.</p> <p>Oh My ZSH! is a delightful, open source, community-driven framework for managing your Zsh configuration. It comes bundled with thousands of helpful functions, helpers, plugins, themes, and a few things that make you shout... <code>\"Oh My ZSH!\"</code></p> <p>There is so much in Oh My ZSH! that nobody use it all. But even if using only 10% of it, that is already a giant benefit for productivity. As everything comes bundles in one package with good documentation and community support, it is faster to use it than manage each aspect of the configuration by hand or with different tools.</p> <p>When you start using Oh My ZSH! you can't go back.</p>"},{"location":"shell_setup/#install","title":"Install","text":"<pre><code>sh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n</code></pre> <p>updates</p> <p>Oh My ZSH! checks for the latest version and will warn if the version is outdated every time a new shell is started</p>"},{"location":"shell_setup/#theme","title":"Theme","text":""},{"location":"shell_setup/#powerlevel10k","title":"PowerLevel10k","text":"<p>There are many great themes out there to help developers to become more productive and make the Terminal look cool. On such theme is <code>PowerLevel10k</code>. It's easy to install and provides a ton of tooling in your shell. It's available in a git repo that you have to clone in your <code>.oh-my-zsh</code>  custom folder.</p> <p>Fonts</p> <p>When using iTerm2 or Termux, p10k configure can install the recommended font automatically. Simply answer Yes when asked whether to install Meslo Nerd Font.</p> <p>Else, read the docs to learn how to install the needed fonts.</p> OSXLinux <pre><code>git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k\nsed -i ' ' -e 's/^ZSH_THEME=\".*\"/ZSH_THEME=\"powerlevel10k\\/powerlevel10k\"/g'  ~/.zshrc\nzsh\n</code></pre> <pre><code>git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k\nsed 's/^ZSH_THEME=\".*\"/ZSH_THEME=\"powerlevel10k\\/powerlevel10k\"/g' -i ~/.zshrc\nzsh\n</code></pre> <p>When opening a ZSH shell for the first time (done by calling <code>zsh</code> above), a menu will appear and ask some questions to setup the shell to your convenience.</p> <p>This setup phase can be restarted anytime with the command <code>p10k configure</code></p> <p>Here are the answers to get a cool but productive shell:</p> <ul> <li><code>prompt style</code>: <code>rainbow</code> is generally the best look(3)</li> <li><code>Character Set</code>: <code>Unicode</code> (1)</li> <li><code>Show current time</code>: use 24h (2)</li> <li><code>Prompt Separators</code>: Angled (1)</li> <li><code>Prompt Heads</code>: Sharp (1)</li> <li><code>Prompt Tails</code>: Flat (1)</li> <li><code>Prompt Height</code>: using only one line is better when copy/pasting (1)</li> <li><code>Prompt Connection</code>: (if <code>Prompt Height</code> is set to 2): Disconnected (1)</li> <li><code>Prompt Frame</code>: (if <code>Prompt Height</code> is set to 2): No Frame (1)</li> <li><code>Prompt Spacing</code>: Compact (1)</li> <li><code>Icons</code>: Many (2)</li> <li><code>Prompt Flow</code>: Concise (1)</li> <li><code>Enable Transient Prompt?</code>: Yes (y)</li> <li><code>Instant Prompt Mode</code>: speed up slow scripts... set to Verbose as recommended (1)</li> <li><code>Apply changes to ~/.zshrc</code>: Damn YES (y)</li> </ul>"},{"location":"shell_setup/#agnoster","title":"Agnoster","text":"<p>Agnoster is another cool theme, but less powerful than PowerLevel10k.</p> <p>Agnoster is a lighter theme, less intrusive by default, but lack some of the cool stuff.</p>"},{"location":"shell_setup/#plugins","title":"Plugins","text":"<p>ZSH uses plugins that extend the shortcuts and other behaviours for default apps. Here's some that are worth using:</p> <pre><code>plugins=(brew git python tmux vault terraform)\n</code></pre> <p>Once those plugins are installed and the shell is reloaded, a ton of new Aliases are created. Use <code>alias</code> command to list them all. Here are a few:</p> <pre><code>alias | grep git\n\ng=git\nga='git add'\ngaa='git add --all'\ngam='git am'\n</code></pre>"},{"location":"shell_setup/#im-still-typing-too-much-kubectl","title":"I'm still typing too much <code>kubectl</code>","text":"<p>When working with Kubernetes, some commands are regularily used, like <code>k get po</code>. Even if it is faster to type than <code>kubectl get pods</code>, it's still 8 characters long (including spaces).</p> <p>Oh My ZSH! includes a plugin for <code>kubectl</code> that ads a ton of aliases specific to <code>kubectl</code></p> <p>Those alias are usually based on the first letters of each components of the command to execute, like <code>kgp</code> for <code>Kubectl Get Pods</code>.</p> <p>Simply add <code>kubectl</code> to the list of plugins in <code>.zshrc</code> file:</p> <pre><code>plugins=(brew git python terraform kubectl zsh-autosuggestions)\n</code></pre> <p>Check that all the new aliases are created after starting a new shell:</p> <p><pre><code>alias | grep kubectl\n</code></pre> <pre><code>k=kubectl\nkaf='kubectl apply -f'\nkca='_kca(){ kubectl \"$@\" --all-namespaces;  unset -f _kca; }; _kca'\nkccc='kubectl config current-context'\n...\n</code></pre></p> <p>Because the plugin <code>kubectl</code> already create the alias <code>k=kubectl</code>, we don't need it explicitely in our <code>.zshrc</code> file. </p> <p>Oh My ZSH! also initialize the completion system, so you can also remove it. Comment those lines in the <code>.zshrc</code> file:</p> <pre><code># k=kubectl\n# autoload -Uz compinit\n# compinit\n</code></pre> <p>Note that the aliases are all calling <code>kubectl</code> explicitelly.</p>"},{"location":"shell_setup/#zsh-autosuggestions-plugin","title":"zsh-autosuggestions plugin","text":""},{"location":"shell_setup/#dynamic-prompt","title":"Dynamic Prompt","text":"<p>Oh My ZSH! now includes a neat feature called the <code>dynamic prompt</code>. Instead of having a prompt line which actually takes 3 lines and display your Git repo, Kubernetes Context, AWS or GCP account all the time, the needed information can be displayed only when needed.</p> <p>In the case of Kubernetes, it means only when using the <code>kubectl</code> command, or any associated or similar commands like <code>k</code>.</p> <p>It is possible to add more commands that will trigger the display of the prompt section by editing the <code>~/.p10k.zsh</code> file. Search for the variable <code>POWERLEVEL9K_KUBECONTEXT_SHOW_ON_COMMAND</code> and edit the line.</p> <pre><code>  typeset -g POWERLEVEL9K_KUBECONTEXT_SHOW_ON_COMMAND='kubectl|helm|kubens|kubectx|oc|istioctl|kogito|k9s|helmfile|flux|fluxctl|stern|kubeseal|kubecolor|skaffold'\n</code></pre>"},{"location":"shell_setup/#reference","title":"Reference","text":"<ul> <li>Oh My ZSH !</li> <li>PowerLevel10k</li> </ul>"},{"location":"vibecoding1/","title":"Gemini CLI Tutorial Series","text":"<p>By Romin Irani on Google Cloud - Community 11 min read \u00b7 Jun 27, 2025</p> <p>Welcome to the Gemini CLI Tutorial Series. It is an open-source AI agent that brings the power of Gemini directly into your terminal. You can use it for both coding related tasks as well as other tasks, it comes integrated with various tools, plus support for MCP servers and most importantly, its open source and comes with a generous free tier (60 requests/min and 1,000 requests/day with personal Google account).</p> <p>You can read about the announcement blog post and bookmark the Github page for the project, that contains the latest documentation.</p> <ul> <li>Announcement blog post</li> <li>Github Project</li> </ul> <p>You would have definitely come to this series after reading/hearing about it and hence we will dive straight into putting it to work.</p> <p>Let's go.</p>"},{"location":"vibecoding1/#gemini-cli-tutorial-series_1","title":"Gemini CLI Tutorial Series:","text":"<ul> <li>Part 1 : Installation and Getting Started (this post)</li> <li>Part 2 : Gemini CLI Command line options</li> <li>Part 3 : Configuration settings via settings.json and .env files</li> <li>Part 4 : Built-in Tools</li> <li>Part 5: Using Github MCP Server</li> <li>Part 6: More MCP Servers : Firebase, Google Workspace, Google Gen AI Media Services and MCP Toolbox for Databases</li> <li>Part 7: Custom slash commands</li> <li>Part 8: Building your own MCP Server</li> <li>Part 9: Understanding Context, Memory and Conversational Branching</li> <li>Part 10: Gemini CLI and VS Code Integration</li> <li>Part 11: Gemini CLI Extensions</li> <li>Part 12: Gemini CLI GitHub Actions</li> <li>\u27a1\ufe0f Codelab : Hands-on Gemini CLI</li> </ul>"},{"location":"vibecoding1/#blog-post-updates","title":"Blog post updates:","text":"<p>September 10, 2025 - Screenshots as of Gemini CLI version 0.4.0. Prerequisite for Node version bumped to 20+. - Validated content and output based on the commands used in the article - Clearer and added explanations in a few places.</p>"},{"location":"vibecoding1/#installation-and-getting-started","title":"Installation and Getting Started","text":"<p>In the first part of the tutorial series, we will look at setting up our machine for Gemini CLI. Additionally, we will look at getting started with a basic prompt or two and see how it all works. We will dive deeper into configuration, context and lot more settings for this tool, but in this part, we will keep things light and just go through a few initial steps.</p> <p></p> <p>If you are not looking to install this, consider using Gemini CLI from Cloud Shell. Cloud Shell comes preinstalled with Gemini CLI. You will need to have a Google Cloud Project with billing enabled and know how to navigate your way through Google Cloud console and can launch the Cloud Shell.</p> <p>If you prefer this method, simply skip the next section i.e. Installation and setup.</p>"},{"location":"vibecoding1/#installation-and-setup","title":"Installation and setup","text":"<p>The steps to install Gemini CLI are straightforward. You require Node.js version 20 or higher installed on your machine first. Visit the link and go through with the suggested steps for your Operating System. A sample screenshot for that is shown below:</p> <p></p> <p>Ensure that the versions are correct and then install Gemini CLI as per the command given below:</p> <pre><code>npm install -g @google/gemini-cli\n</code></pre> <p>Once done, I suggest that you check the Gemini CLI version as follows:</p> <pre><code>gemini -v\n</code></pre> <p>On my system (at the time of writing), it shows the following: <code>0.4.0</code></p> <p>Go ahead and launch Gemini CLI via the <code>gemini</code> command. Keep in mind that this is a client running in your terminal, so be comfortable with using the keyboard (Arrow keys, etc).</p> <p>It would first ask you about choosing a theme. Go ahead and select one that you like:</p> <p></p> <p>Once selected, it asks you to select the Authentication method as shown below:</p> <p></p> <p>Go with the Google login, which will provide you access to the free tier of Gemini CLI, which allows for 60 requests/minute, 1000 model requests per day. This will invoke the browser, where you will need to login with your Google credentials for the account that you wish to use here. Once done, you should see <code>Gemini CLI</code> waiting for your command.</p> <p></p> <p>Alternately, if you need higher quota, feel free to provide your Gemini API Key or even Vertex AI, where you may have a Google Cloud Project with billing enabled. Do refer to the Authentication section of the documentation.</p>"},{"location":"vibecoding1/#gemini-cli-terminal-interface","title":"Gemini CLI Terminal Interface","text":"<p>Few things to immediately notice in the <code>Gemini CLI</code> terminal interface:</p> <ol> <li>Observe the bottom status bar. It has the currently folder shown to the left. The model (<code>gemini-2.5-pro</code>) which the CLI will use and the amount of context that is remaining. It is not running in a sandbox, which is a mechanism to isolate potentially dangerous operations (such as shell commands or file modifications) from your host system, providing a security barrier between AI operations and your environment.</li> <li>Type <code>/help</code> (forward slash) and you will see a variety of commands, keyboard shortcuts and how to use <code>@</code> to specific files/folders that you would like to have in the context: Notice that you have the <code>auth</code> and the <code>theme</code> command that you will be familiar with since you did actually execute that at the time of installation. In case you wish to change the theme or the authentication method, you can invoke them anytime.</li> </ol> <p></p> <ol> <li>Need to refer the Gemini CLI documentation page, use the <code>/docs</code> command.</li> <li>If you'd like to learn about your current session statistics like <code>total tokens</code>, <code>duration</code>, etc \u2014 you can use the <code>/stats</code> command.</li> </ol> <p>If you'd like to quit Gemini CLI, you can use the <code>/quit</code> command or press <code>Ctrl-C</code> twice.</p>"},{"location":"vibecoding1/#our-first-words-with-gemini-cli","title":"Our first words with Gemini CLI","text":"<p>Notice that you can simply type in your message now and see <code>Gemini CLI</code> come to work for you. Let's do that.</p> <p>Let's ask for help first and you guessed it correct, just give <code>help</code> without the forward slash. It did satisfactorily well to come up with this response, even though we did not provide it much context:</p> <p></p> <p>As you would know by now, that Gemini CLI is an AI Agent integrated into your terminal, which means that it will be interacting with your environment to complete some of the tasks. This would include having access to your file system, reading/writing files and a lot more. We will be covering more of the tools that Gemini CLI comes wih, but for now you can get a list of all the tools (not MCP Server tools) that are available to Gemini. Just give the <code>/tools</code> command, the output of which is given below:</p> <p></p> <p>What this means is that Gemini CLI will use any of these tools as required but do not worry at this point about it going all crazy with these tools without your permission to do things. You will notice that a tool usage will first result in Gemini prompting you for your permission to invoke the tool. We shall see this later in the series.</p> <p>These tools are likely to increase in number or undergo changes in the way they work. Note that these are tools that come inbuilt with Gemini CLI. You can further expand Gemini CLI's capabilities by integrating MCP Servers into the CLI and also writing your own custom commands. We shall look at those later in the series.</p> <p>Out of curiosity, I ask it what the <code>GoogleSearch</code> tool does:</p> <p></p> <p>Let's give that a try with finding the latest news on India Cricket:</p> <p></p> <p>Before we go ahead and give a command to Gemini CLI to create some content/application, it helps to understand the current working directory/folder again. Notice that at the bottom of the status bar to the left is the current folder. Not sure, what it is, you can swap to Shell mode by simply typing in <code>!</code>, this will toggle to shell mode as shown below.</p> <p>The Shell or passthrough command is very interesting and it allow you to interact with your system's shell directly from within Gemini CLI.</p> <pre><code>! pwd\n</code></pre> <p>It says shell mode enabled and I am typing the <code>pwd</code> command to understand where I am. The output is shown as below:</p> <p></p> <p>You can come out of the terminal mode, by hitting <code>ESC</code> key.</p> <p>You can quit Gemini CLI via the <code>/quit</code> command and then ensure that you are in the folder that you would like to be and then launch <code>gemini</code> from there.</p>"},{"location":"vibecoding1/#start-gemini-cli-from-the-folder-youd-like-to-start-with","title":"Start Gemini CLI from the folder you'd like to start with","text":"<p>Gemini CLI is a project based tool and expects that you typically start it from a folder in which you want to work in. So ideally you do not want to be in the home folder or at the root of some tree and then expect to go down into another folder. So be cognizant of this and launch Gemini CLI from a folder that you want to work from.</p> <p>. I have created a root folder named <code>/gemini-cli-projects</code> in this folder. I then have multiple other folders that are like mini-projects that I am working on. For e.g. for this series I have created a folder inside of that named <code>/cli-series</code> and I am working off that.</p>"},{"location":"vibecoding1/#vibe-coding-task","title":"Vibe coding task","text":"<p>Let's give Gemini CLI our first task and ask it to build a web application that displays the content of a current RSS feed. Say I follow Cricket with keen interest and for some reason, I am interested in viewing live scores of any match that is going on. Not sure why, but let's leave that here.</p> <p>Cricinfo.com provides a live feed of cricket scores over here: <code>https://static.cricinfo.com/rss/livescores.xml</code>. Let's put Gemini CLI to the test and see how well it does on a sample web application that I'd like to create. Let's go step by step.</p> <p>Paste following prompt example:</p> <pre><code>I would like to create a Python Flask Application that shows me a list of live scores of cricket matches. There is a RSS Feed for this that is available over here: `https://static.cricinfo.com/rss/livescores.xml`. Let's use that.\n</code></pre> <p>Gemini CLI responds with the following:</p> <p></p> <p>You can see that it is planning to use one of the tools to create a directory, for which it needs permission to run the <code>mkdir</code> command. This is a recurrent pattern that you will see, where you can either allow the tool once, always allow and so on. I am going to allow these tools for now since it is being transparent with what it is going to create, i.e. a sub-folder named <code>cricket-scores-app</code> in my <code>gemini-cli-projects/cli-series</code>, which is the folder from which I launched the Gemini CLI.</p> <p>Once it created the folders and touched the files required, it is now beginning to generate code for the application and is using the <code>WriteFile</code> tool to create the file + contents, for which it needs permission too.</p> <p></p> <p>Its then gone ahead and created the Python file, HTML file, its identified the Python requirements (<code>Flask</code>, <code>requests</code> and <code>feedparser</code> packages) and is now asking me if it can install the Python packages via <code>pip</code>.</p> <p></p> <p>It then goes ahead and creates a virtual environment in Python to setup the dependencies. I give it the permissions and it goes about merrily with the next steps to install the dependencies in the environment and starting the Flask Server on port 5000. It gave an error saying that port 5000 is already in use, so I hit <code>ESC</code> and asked it to modify the server code to run on port 7000 instead.</p> <p></p> <p>Once the changes were done, it was able to launch the server successfully.</p> <p></p> <p>I visit the application locally at <code>http://127.0.0.1:7000</code>, a screenshot of which is shown below:</p> <p></p> <p>The code generated for the Flask Application was simple and straightforward:</p> <p>app.py <pre><code>import flask\nimport feedparser\nimport requests\n\napp = flask.Flask(__name__)\n\ndef get_live_scores():\n  \"\"\"Fetches and parses live cricket scores from the RSS feed.\"\"\"\n  rss_url = \"https://static.cricinfo.com/rss/livescores.xml\"\n  try:\n    response = requests.get(rss_url)\n    response.raise_for_status() # Raise an exception for bad status codes\n    feed = feedparser.parse(response.content)\n    return feed.entries\n  except requests.exceptions.RequestException as e:\n    print(f\"Error fetching RSS feed: {e}\")\n    return []\n\n@app.route('/')\ndef index():\n  \"\"\"Renders the index page with live scores.\"\"\"\n  scores = get_live_scores()\n  return flask.render_template('index.html', scores=scores)\n\nif __name__ == '__main__':\n  app.run(debug=True, port=7000)\n</code></pre></p> <p>templates/index.html <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\"&gt;\n  &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n  &lt;title&gt;Live Cricket Scores&lt;/title&gt;\n  &lt;link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;div class=\"container\"&gt;\n    &lt;h1 class=\"mt-5\"&gt;Live Cricket Scores&lt;/h1&gt;\n    &lt;ul class=\"list-group mt-3\"&gt;\n      {% for score in scores %}\n        &lt;li class=\"list-group-item\"&gt;{{ score.title }}&lt;/li&gt;\n      {% endfor %}\n    &lt;/ul&gt;\n  &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>We have hardly scratched the surface here but feel free to take it for a spin. Try out some tasks or an application or two that you would like to see getting generated. It's early days, so be ready for some surprises. I have found that I usually end up prompting again with more context or asking it to do better and more.</p> <p>As we progress through this lavs, we will see a lot more of configuration, context setting and then when we come to various use cases, we will look at developing applications from scratch, migrating current applications, adding a feature or two to an existing application and more. So stay tuned.</p>"},{"location":"cleanup/cleanup/","title":"Cleanup","text":"<p>Here's a script you can run to clean up your computer from all the stuff we installed during this tutorial.</p> Apple MacLinuxWindows macCleanup.sh<pre><code>#!/bin/sh\n#\n# This is the cleanup script for Apple Mac OS\n\n# Kubectl and tooling\nrm -f /usr/local/bin/kubectl\nrm -f /usr/local/bin/kubectl-krew\nrm -f /usr/local/bin/kubie\nrm -f /usr/local/bin/stern\n\n# Kind\nkind delete cluster --name dev\nkind delete cluster --name stg\nbrew uninstall kind\n\n# remove Minikube\nminikube stop\nminikube delete\nrm -f /usr/local/bin/minikube\n\n# remove brew installed files\nbrew uninstall kubectl\n\n# stop Colima and remove it\ncolima kubernetes delete\ncolima stop\n\nbrew uninstall colima\nbrew uninstall docker\n\n# Remove Podman\npodman machine stop\npodman machine rm\nbrew uninstall podman\nbrew uninstall podman-desktop\n\n# Remove tooling\nbrew uninstall kubecolor/tap/kubecolor\n\n# Remove ZSH setup\nrm -f ~/.p10k.zsh\nmv -f ~/.zshrc ~/.zshrc-demo\nmv -f ~/.zshrc.pre-oh-my-zsh ~/.zshrc\n</code></pre> linuxCleanup.sg<pre><code>#!/bin/sh\n#\n# This is the cleanup script for Linux\n\n\n# Kind\nkind delete cluster demo\nrm -f /usr/local/bin/kind\n\n# remove Minikube\nminikube stop\nminikube delete\nrm -f /usr/local/bin/minikube\n\n# remove kubectl and tools\nrm -f /usr/local/bin/kubectl\nrm -f /usr/local/bin/kubecolor\nrm -f /usr/local/bin/kubectl-krew\nrm -f /usr/local/bin/kubie\n\n# stop Colima and remove it\ncolima kubernetes delete\ncolima stop\n\n# stop and delete Podman\n# TODO\n\n# Remove K3s\n# TODO: delete the cluster and remove VMs files\n/usr/local/bin/k3s-uninstall.sh\n\n# Remove ZSH setup\nrm -f ~/.p10k.zsh\nmv -f ~/.zshrc ~/.zshrc-demo\nmv -f ~/.zshrc.pre-oh-my-zsh ~/.zshrc\n</code></pre> windowsCleanup.sg<pre><code>#!/bin/sh\n#\n# This is the cleanup script for Windows\n</code></pre>"},{"location":"dev_tooling/apps/","title":"Other cool apps for Kubernetes","text":"<p>Here is a list of some more apps that a cool to have.</p>"},{"location":"dev_tooling/apps/#kubepug-deprecations","title":"KubePug / Deprecations","text":"<p>KubePug/Deprecations is intended to be a kubectl plugin, which:</p> <ul> <li>Downloads a swagger.json from a specific Kubernetes version</li> <li>Parses this Json finding deprecation notices</li> <li>Verifies the current kubernetes cluster or input files checking whether exists objects in this deprecated API Versions, allowing the user to check before migrating</li> </ul> <p>This is a tool that you can use locally or in your CI to ensure you're not working with deprecated resources.</p>"},{"location":"dev_tooling/apps/#install","title":"install","text":"<pre><code>kubectl krew install deprecations\n</code></pre>"},{"location":"dev_tooling/apps/#usage","title":"Usage","text":"<ul> <li> <p>Full cluster scan</p> <p>This will walk you through all your k8s resources (in the local context) and compare them to the target version (here 1.25). You will get warnings if you still have resources using deprecated APIs :</p> <pre><code>kubectl deprecations --k8s-version v1.25.0\n</code></pre> </li> <li> <p>File scan</p> <p>This command will look at the resources inside the <code>../yaml/sample_app.yaml</code> and compare it to the Kubernetes API version <code>v1.25.0</code>. It will report every resource that is outdated:</p> <pre><code>kubectl deprecations --k8s-version v1.25.0 --input-file ../yaml/sample_app.yaml\n</code></pre> </li> </ul>"},{"location":"dev_tooling/apps/#dive","title":"Dive","text":"<p>Dive is an utility to inspect Docker Images. While it is not directly used with Kubernetes, it can help improve size and security in the pod's images.</p>"},{"location":"dev_tooling/apps/#install_1","title":"Install","text":"<pre><code>brew install dive\n</code></pre> <p>Note</p> <p>If using <code>podman</code> or any Docker-For-desktop replacement, you need to ensure <code>Dive</code> will be able to connect to the Docker Daemon. Set <code>DOCKER_HOST</code> variable accordinately:</p> <pre><code>export DOCKER_HOST=\"unix://$HOME/.local/share/containers/podman/machine/podman-machine-default/podman.sock\"\n</code></pre>"},{"location":"dev_tooling/apps/#usage_1","title":"Usage","text":"<ul> <li> <p>check Alpine image</p> <pre><code>dive alpine:latest\n</code></pre> <p></p> </li> </ul>"},{"location":"dev_tooling/apps/#dasel","title":"Dasel","text":"<p>Use Dasel to query and modify data structures using selector strings.</p> <p>It is Comparable to jq / yq, but supports JSON, YAML, TOML, XML and CSV with zero runtime dependencies.</p> <p>This is the swiss army knife of the structrued file (in the command line).</p>"},{"location":"dev_tooling/apps/#install_2","title":"Install","text":"Apple MacOS XGo <pre><code>brew install dasel\n</code></pre> <pre><code>go install github.com/tomwright/dasel/cmd/dasel@master\n</code></pre>"},{"location":"dev_tooling/apps/#usage_2","title":"Usage","text":"<p>Here's some sample usage, taken from the doc:</p> <pre><code># Pretty Print JSON\necho '[{\"name\": \"Tom\"},{\"name\": \"Paul\"}]' | dasel -p json\n[\n  {\n    \"name\": \"Tom\"\n  },\n  {\n    \"name\": \"Paul\"\n  }\n]\n\n# JSON to YAML\necho '[{\"name\": \"Tom\"},{\"name\": \"Paul\"}]' | dasel -r json -w yaml\n- name: Tom\n- name: Paul\n</code></pre> <p>While this is some good example, here's some real-case usage with K8s.</p> <p>Let's create a sample deployment file:</p> <pre><code>cat &gt; deployment.yaml &lt;&lt; EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: example-app\n  namespace: test\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n    app: example-app\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: example-app\n    spec:\n      containers:\n      - image: alpine:latest\n        imagePullPolicy: Always\n        name: manager\n        ports:\n        - containerPort: 8080\n          name: web\n          protocol: TCP\nEOF\n</code></pre> <p>Warning</p> <p>It is important to keep the <code>.yaml</code> extension of the file as <code>Dasel</code> use it to auto-find the right language. If not, you can force it using the <code>-p</code> or <code>--parser</code> flag, or the individual <code>--read</code> and <code>--write</code>.</p> <ul> <li> <p>Select the image of the <code>manager</code> container</p> <p><pre><code>dasel -p yaml select -f deployment.yaml -s \"spec.template.spec.containers.(name=manager).image\"\n</code></pre> <pre><code>alpine\n</code></pre></p> </li> <li> <p>Change the image of the <code>manager</code> containe in-place</p> <pre><code>dasel put string -f deployment.yaml -s \"spec.template.spec.containers.(name=manager).image\" \"ubuntu:latest\"\n</code></pre> </li> <li> <p>Update replicas to 3 in-place</p> <pre><code>dasel put int -f deployment.yaml -s \"spec.replicas\" 3\n</code></pre> </li> <li> <p>Add a new env var to the deployment</p> <pre><code>dasel put object -f deployment.yaml -s \"spec.template.spec.containers.(name=manager).env.[]\" -t string -t string name=MY_NEW_ENV_VAR value=MY_NEW_VALUE\n</code></pre> </li> <li> <p>Update an existing env var</p> <pre><code>dasel put string -f deployment.yaml -s \"spec.template.spec.containers.(name=manager).env.(name=MY_NEW_ENV_VAR).value\" NEW_VALUE\n</code></pre> </li> </ul> <p>Warning</p> <p>The above commands are not fail-proof, and executing <code>dasel put</code> multiple times may generate invalid yaml.</p>"},{"location":"dev_tooling/apps/#next","title":"Next","text":"<p>This is the end of the tutorial. If you want, use the Cleanup script to remove most of what you installed.</p>"},{"location":"dev_tooling/vscode/","title":"VsCode Extensions","text":"<p>VSCode is widely used as an editor and there\u2019s a few, well, there\u2019s a ton of extensions that can enhence the productivity. Here are a few.</p>"},{"location":"dev_tooling/vscode/#kubernetes-extension","title":"Kubernetes extension","text":"<p>The Kubernetes extension brings with it all the highlighting, linting and documenting of k8s resources, plus a lot of commands to work with Docker, K8s, Helm\u2026 it\u2019s actually far too much to use it all.</p> <p></p>"},{"location":"dev_tooling/vscode/#yaml-extension","title":"YAML extension","text":"<p>The YAML extension is, well, just about yaml. This one is mandatory and it's usually already installed ! </p>"},{"location":"dev_tooling/vscode/#indent-rainbow-extensions","title":"Indent-Rainbow extensions","text":"<p>One of the biggest problems with YAML is indentation\u2026 you miss one, yaml is not valid. </p> <p>When you have 1000+ lines of yaml, go find the error\u2026 and this is less uncommon than you may think.</p> <p>Indent-Rainbow extension color-code the tabs in your YAMLs so it\u2019s visually easy to see how many tabs you have in your file.</p> <p>This is totally a productivity tool that every developper needs.</p> <p>(Thank you @melmaliacone for sharing this)</p> <p></p>"},{"location":"dev_tooling/vscode/#next","title":"Next","text":"<p>Learn about other cool apps in the next chapter.</p>"},{"location":"helm/helm/","title":"Helm","text":"<p>Helm is templating tool for yaml, aimed for Kubernetes.</p> <p>Helm is used to create (or use) <code>packages</code> of you application called a <code>Chart</code>. Most Kubernetes applications like the CNCF apps provides a Helm chart with default values. It is then possible to tune the values to suite a special environment or special need.</p> <p>Charts are available to be downloaded and used locally or directly from a remote URL or repository.</p>"},{"location":"helm/helm/#install","title":"Install","text":"Apple Mac OS XFrom Release page <pre><code>brew install helm\n</code></pre> <p>Download the right version from the Projet's Release page.</p>"},{"location":"helm/helm/#usage","title":"Usage","text":""},{"location":"helm/helm/#managing-remote-helm-repository","title":"Managing Remote Helm repository","text":"<p><pre><code>helm repo list\n</code></pre> output<pre><code>NAME                    URL\nstable                  https://charts.helm.sh/stable\nhashicorp               https://helm.releases.hashicorp.com\nkiali                   https://kiali.org/helm-charts\ncouchbase               https://couchbase-partners.github.io/helm-charts/\nsumologic               https://sumologic.github.io/sumologic-kubernetes-collection\ncilium                  https://helm.cilium.io/\nprometheus-community    https://prometheus-community.github.io/helm-charts\n</code></pre></p>"},{"location":"helm/helm/#deploy-an-existing-chart","title":"Deploy an existing chart","text":"<p>To deploy a chart, the repository needs to be added (if not using a local chart):</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n</code></pre> <p>Then we can check the <code>values</code> that can be tuned in the chart:</p> <p><pre><code>helm show values bitnami/mysql\n</code></pre> output<pre><code>server:\n  ## Prometheus server container name\n  ##\n  enabled: true\n\n  ## Use a ClusterRole (and ClusterRoleBinding)\n  ## - If set to false - we define a RoleBinding in the defined namespaces ONLY\n  ##\n  ## NB: because we need a Role with nonResourceURL's (\"/metrics\") - you must get someone with Cluster-admin privileges to define this role for you, before running with this setting enabled.\n  ##     This makes prometheus work - for users who do not have ClusterAdmin privs, but wants prometheus to operate on their own namespaces, instead of clusterwide.\n  ##\n  ## You MUST also set namespaces to the ones you have access to and want monitored by Prometheus.\n  ##\n  # useExistingClusterRoleName: nameofclusterrole\n\n  ## namespaces to monitor (instead of monitoring all - clusterwide). Needed if you want to run without Cluster-admin privileges.\n  # namespaces:\n  #   - yournamespace\n\n  name: server\n\n  # sidecarContainers - add more containers to prometheus server\n  # Key/Value where Key is the sidecar `- name: &lt;Key&gt;`\n  # Example:\n  #   sidecarContainers:\n  #      webserver:\n  #        image: nginx\n  sidecarContainers: {}\n\n  ## Prometheus server container image\n  ##\n  image:\n    repository: quay.io/prometheus/prometheus\n    tag: v2.39.1\n    pullPolicy: IfNotPresent\n\n  ## prometheus server priorityClassName\n...\n</code></pre></p> <p>The chart creates mysql node with 256Mi of memory and 250m CPU by default. This is too much for our cluster, so let's bring that down.</p> <p>Using <code>helm template</code> it is possible to generate the final <code>yaml</code> before applying it:</p> <pre><code>helm template prometheus prometheus-community/prometheus \\\n  -n monitoring \\\n  --set alertmanager.enabled=false \\\n  --set persistentVolume.enabled=false \\\n  --set nodeExporter.enabled=true \\\n  --create-namespace\n</code></pre> <p>Then it is possible to deploy the chart using:</p> <pre><code>helm install prometheus prometheus-community/prometheus \\\n  -n monitoring \\\n  --set alertmanager.enabled=false \\\n  --set persistentVolume.enabled=false \\\n  --set nodeExporter.enabled=true \\\n  --create-namespace\n</code></pre> <p>Check the pod is running:</p> <p><pre><code>kubectl get pods -n monitoring\n</code></pre> output<pre><code>NAME                                             READY   STATUS    RESTARTS   AGE\nprometheus-kube-state-metrics-774f8c7564-t5k9s   1/1     Running   0          110s\nprometheus-node-exporter-slv85                   1/1     Running   0          110s\nprometheus-pushgateway-5957cfcf57-xwczj          1/1     Running   0          110s\nprometheus-server-6bd54674cc-x4lm6               1/2     Running   0          110s\n</code></pre></p>"},{"location":"helm/helm/#updating-a-release","title":"Updating a release","text":"<p>After some time the chart may be updated, a new version may come out, or we need to update some values.</p> <p>Before an upgrade, it is better to check what the change will be. The Helm-Diff plugin can ease that:</p> <pre><code>helm plugin install https://github.com/databus23/helm-diff\n</code></pre> <p>Check the upgrade differences:</p> <p><pre><code>helm diff upgrade prometheus prometheus-community/prometheus \\\n  -n monitoring \\\n  --set alertmanager.enabled=false \\\n  --set persistentVolume.enabled=false \\\n  --set nodeExporter.enabled=true \\\n  --set server.resources.limits.memory=\"256Mi\" \\\n  --set server.resources.limits.cpu=\"300m\" \n</code></pre> output<pre><code>monitoring, prometheus-server, Deployment (apps) has changed:\n # Source: prometheus/templates/server/deploy.yaml\n  apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    name: prometheus-server\n    namespace: monitoring\n  spec:\n...\n    replicas: 1\n    template:\n      spec:\n        enableServiceLinks: true\n        serviceAccountName: prometheus-server\n        containers:\n          - name: prometheus-server-configmap-reload\n...\n\n          - name: prometheus-server\n            image: \"quay.io/prometheus/prometheus:v2.39.1\"\n            imagePullPolicy: \"IfNotPresent\"\n            args:\n              - --storage.tsdb.retention.time=15d\n              - --config.file=/etc/config/prometheus.yml\n              - --storage.tsdb.path=/data\n              - --web.console.libraries=/etc/prometheus/console_libraries\n              - --web.console.templates=/etc/prometheus/consoles\n              - --web.enable-lifecycle\n            ports:\n              - containerPort: 9090\n            resources:\n-             {}\n+             limits:\n+               cpu: 300m\n+               memory: 256Mi\n...\n</code></pre></p> <p>Re-deploy with changing some parameters:</p> <pre><code>helm upgrade prometheus prometheus-community/prometheus \\\n  -n monitoring \\\n  --set alertmanager.enabled=false \\\n  --set persistentVolume.enabled=false \\\n  --set nodeExporter.enabled=true \\\n  --set server.resources.limits.memory=\"256Mi\" \\\n  --set server.resources.limits.cpu=\"300m\" \n</code></pre>"},{"location":"helm/helm/#validate-which-version-is-deployed","title":"Validate which version is deployed","text":"<p>When helm deploy a Chart it keeps a trace in a <code>secret</code>. The <code>helm</code> CLI includes some commands that uses the secret to give informations of the status of the deployment. </p> <p>List the deployed charts: <pre><code>helm list -n monitoring\n</code></pre> output<pre><code>NAME          NAMESPACE         REVISION     UPDATED                    STATUS      CHART               APP VERSION\nprometheus  monitoring  2   2022-10-27 12:00:24.649426 -0400 EDT    deployed    prometheus-15.16.1  2.39.1\n</code></pre></p> <p>It is possible to check history revisions:</p> <p><pre><code>helm history prometheus -n monitoring\n</code></pre> output<pre><code>REVISION    UPDATED                     STATUS      CHART               APP VERSION DESCRIPTION\n1           Thu Oct 27 11:51:19 2022    superseded  prometheus-15.16.1  2.39.1      Install complete\n2           Thu Oct 27 12:00:24 2022    deployed    prometheus-15.16.1  2.39.1      Upgrade complete\n</code></pre></p>"},{"location":"helm/helm/#view-deployment-values-and-yaml","title":"View deployment values and yaml","text":"<p>Helm includes commands to see what was used for each revision:</p> <p><pre><code>helm get values prometheus -n monitoring\n</code></pre> output<pre><code>USER-SUPPLIED VALUES:\nalertmanager:\n  enabled: false\nnodeExporter:\n  enabled: true\npersistentVolume:\n  enabled: false\nserver:\n  resources:\n    limits:\n      cpu: 300m\n      memory: 256Mi\n</code></pre></p> <p>Or the generated manifest:</p> <pre><code>helm get manifest prometheus -n monitoring\n</code></pre> <p>The data inside the revision Helm <code>secret</code> is double encoded and gzipped:</p> <p><pre><code>kubectl -n monitoring get secret\n</code></pre> output<pre><code>NAME                               TYPE                 DATA   AGE\nsh.helm.release.v1.prometheus.v1   helm.sh/release.v1   1      22m\nsh.helm.release.v1.prometheus.v2   helm.sh/release.v1   1      13m\n</code></pre></p> <p>If you really like to dump the content for youself:</p> <p>Warning</p> <p>The Secret's data also store some sensible values (secrets) </p> <pre><code>kubectl get secret -n monitoring sh.helm.release.v1.prometheus.v2 \\\n  --output=go-template={{.data.release}}  | base64 --decode | base64 --decode | gunzip\n</code></pre> <p>Check out the Lens UI, which now has a plugin to dig into Helm releases !</p>"},{"location":"interfaces/k9s/","title":"K9s","text":"<p>K9s is a terminal interface which will help get a dynamic view of a cluster.</p> <p>Install is straightforward, it\u2019s a single binary, and it connect to the current context by default</p>"},{"location":"interfaces/k9s/#install","title":"Install","text":"MacOsXGo <pre><code>brew install k9s\n</code></pre> <pre><code># NOTE: The dev version will be in effect!\ngo install github.com/derailed/k9s@latest\n</code></pre> <p>Warning</p> <p>Go install seems to be broken...</p>"},{"location":"interfaces/k9s/#usage","title":"Usage","text":"<p>Here are some of the basic command line options to start <code>k9s</code>:</p> <ul> <li> <p><code>k9s -n &lt;namespace&gt;</code>:      start k9s within a specific namespace</p> </li> <li> <p><code>k9s --context &lt;context&gt;</code>:      start k9s using another context than the selected one</p> </li> <li> <p><code>k9s --readonly</code>:      start k9s in read-only mode, to ensure you won't change any running resource</p> </li> </ul> <p>As <code>k9s</code> is a terminal console application, there's no mouse support or buttons to click. Here is a list of the most used commands you can use:</p> <ul> <li><code>/&lt;filter&gt;</code>: to filter the list of resources, ex: /core to list only coredns pods</li> <li><code>&lt;ESC&gt;</code>: to stop any command or clear filters</li> <li><code>0</code>: to list pods from ALL namespaces</li> <li><code>d</code>: (on a resource): describe the resource</li> <li><code>e</code>: (on a resource): edit the resource (:q to quit, like in VI, depends on your EDITOR variable I guess)</li> <li><code>l</code>: (on a resource): show logs</li> <li><code>?</code>: help with possible commands</li> <li><code>s</code>: (on a resource): open a shell in the container</li> <li><code>&lt;enter&gt;</code>: (on a resource): view the resource content</li> <li><code>y</code>: (on a resource): display yaml of the resource</li> </ul>"},{"location":"interfaces/k9s/#examples","title":"Examples","text":"<ul> <li> <p>Default k9s view</p> <p></p> </li> <li> <p>Describe of a Pods</p> <p></p> </li> </ul>"},{"location":"interfaces/k9s/#next","title":"Next","text":"<p>Leave the shell and use a 100% UI application in next chapter !</p>"},{"location":"interfaces/kubenav/","title":"KubeNav","text":"<p>kubenav is the navigator for your Kubernetes clusters right in your pocket.</p> <p>Kubenav is an opensource UI on top on Kubernetes. Works on Apple iOS, Android and most Desktop OS.</p> <p>It\u2019s a little difficult to configure when using a cloud resource, and the docs are gone at the moment. </p> <p>Check them on Github</p> <p></p> <p></p>"},{"location":"interfaces/kubenav/#next","title":"Next","text":"<p>Next chapter is all about developper productivity !</p>"},{"location":"interfaces/lens/","title":"Lens","text":"<p>Lens is the only IDE you\u2019ll ever need to take control of your Kubernetes clusters:</p> <ul> <li>Launch it on your own desktop</li> <li>Include advanced config to reach remote clusters</li> <li>Manage CustomResourceDefinitions (CRD)</li> </ul> <p>Note</p> <p>While a paid version of Lens exist, you can still use the opensource version. Registering an account (mandatory to use the app) is free.</p>"},{"location":"interfaces/lens/#install","title":"Install","text":"<p>All OS: Download the binary from Lens Website.</p>"},{"location":"interfaces/lens/#example","title":"Example","text":"<ul> <li> <p>Default Lens view</p> <p></p> </li> <li> <p>Lens Workload Overview</p> <p></p> </li> <li> <p>Lens Workload view</p> <p></p> </li> </ul>"},{"location":"interfaces/lens/#next","title":"Next","text":"<p>Another option ? check next chapter !</p>"},{"location":"interfaces/octant/","title":"Octant","text":"<p>Octant is a tool for developers to understand how applications run on a Kubernetes cluster. It aims to be part of the developer's toolkit for gaining insight and approaching complexity found in Kubernetes. Octant offers a combination of introspective tooling, cluster navigation, and object management along with a plugin system to further extend its capabilities.</p> <p>Octant is a Go \"backend\" application that run on your laptop and use a browser to access the service. It use your local Kube Config to discover clusters.</p>"},{"location":"interfaces/octant/#install","title":"Install","text":"Mac Os XLinuxWindows <pre><code>brew install octant\nmkdir -p  ~/.config/octant/plugins\n\noctant\n</code></pre> <p>Download the .deb or .rpm from the releases page.</p> <pre><code>choco install octant --confirm\n</code></pre> <p>Or download a pre-built package from the releases page.</p>"},{"location":"interfaces/octant/#usage","title":"Usage","text":"<p>Octant will open a brower and connect to the Octant App when you start it. </p> <p>You can then browse your clusters and edit the resources.</p> <p>You can install plugins, which are Go binaries.</p> <p></p>"},{"location":"interfaces/octant/#next","title":"Next","text":"<p>Need a mobile phone app ? check next chapter !</p>"},{"location":"kubectl_tooling/krew/","title":"Extending <code>kubectl</code>","text":"<p>Couple years ago K8s community introduced an easy way to extend kubectl via <code>plugins</code>.</p> <p>Plugins are in fact \"applications\" (executable files) named <code>kubectl-&lt;plugin_name&gt;</code>, that are executed when you call <code>kubectl plugin_name</code>.</p> <p><code>Krew</code> is the plugin manager for <code>kubectl</code> command-line tool and it's  maintained by the Kubernetes SIG CLI community.</p> <p>Krew helps you:</p> <ul> <li>discover kubectl plugins,</li> <li>install them on your machine,</li> <li>and keep the installed plugins up-to-date. There are 207 kubectl plugins currently distributed on Krew.</li> </ul>"},{"location":"kubectl_tooling/krew/#krew","title":"Krew","text":"<p><code>Krew</code> is an example of such a kubectl plugin that act as a plugin manager for kubectl. It pre-dates the plugin addition in kubectl and may seem useless now, but it still has it's role to play.</p>"},{"location":"kubectl_tooling/krew/#install","title":"Install","text":"<p>Please refer to the official install doc.</p> Apple MacUsing KrewWindows krew install<pre><code>brew install krew\n</code></pre> <p>This one is using <code>krew</code> itself to install <code>krew</code>.</p> <p>Note</p> <p><code>krew</code> installs plugins in <code>$KREW_ROOT</code> if you set it or <code>$HOME/.krew</code>. You have to then ensure <code>$HOME/.krew</code> is in your path.</p> <p>Add this to your <code>.zshrc</code> or <code>.bashrc</code>:</p> <pre><code>export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\n</code></pre> <pre><code>(\n  set -x; cd \"$(mktemp -d)\" &amp;&amp;\n  OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\n  ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &amp;&amp;\n  KREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\n  curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\n  tar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n  ./\"${KREW}\" install krew\n)\n</code></pre> <ul> <li>Make sure git is installed.</li> <li>Download <code>krew.exe</code> from the Releases page to a directory.</li> <li>Launch a command prompt (<code>cmd.exe</code>) with administrator privileges (since the installation requires use of symbolic links) and navigate to that directory.</li> <li>Run the following command to install krew:     <pre><code>.\\krew install krew\n</code></pre></li> <li>Add the <code>%USERPROFILE%\\.krew\\bin</code> directory to your <code>PATH</code> environment variable (how?)</li> <li>Launch a new command-line window.</li> <li>Run <code>kubectl krew</code> to check the installation.</li> </ul>"},{"location":"kubectl_tooling/krew/#usage","title":"Usage","text":"<pre><code>kubectl krew update\n</code></pre> <p><pre><code>kubectl krew list\n</code></pre> output<pre><code>PLUGIN  VERSION\nctx     v0.9.4\nkrew    v0.4.1\nns      v0.9.4\nwhoami  v0.0.36\n</code></pre> <pre><code>kubectl krew search\n</code></pre> output<pre><code>NAME                            DESCRIPTION                                         INSTALLED\naccess-matrix                   Show an RBAC access matrix for server resources     no\nblame                           Show who edited resource fields.                    no\ncert-manager                    Manage cert-manager resources inside your cluster   no\nctx                             Switch between contexts in your kubeconfig          yes\n...\n</code></pre></p>"},{"location":"kubectl_tooling/krew/#krew-plugins","title":"Krew Plugins","text":"<p>Install plugins that will be used in the tutorial:</p> <ul> <li>ctx: current cluster <code>Context</code> and quick context changes</li> <li>ns: current <code>Namespace</code> and quick namespace changes</li> <li>whoami: who the cluster thinks you are from your authentication</li> <li>who-can: RBAC rules introspection</li> <li>view-secret: directly view secret content without having to decode</li> </ul> <p>Install them with this command:</p> <pre><code>kubectl krew install neat ctx ns whoami who-can view-secret\n</code></pre>"},{"location":"kubectl_tooling/krew/#generating-the-application-manifest","title":"Generating the application manifest","text":"<p>The GoWebApp application was previously deployed with a hardcoded password for Mysql. This is not optimal and not recommended. It was done like this because it's a DEMO, and not real production cluster.</p> <p>Note</p> <p>Usually all application's deployment files (the YAML) should be managed in a versioned repository and should never be modified directly on the cluster.</p> <p>For this DEMO, still, we are going to use the currently deployed application and modify it.</p> <p>Now that the application is running and everything is fine, it is a good idea to store the resulting yaml in our own repo.</p> <p>First dump the <code>gowebapp</code> deployment into a file. By using the <code>--output yaml</code> (<code>-o yaml</code>) option, <code>kubectl</code> will dump the full file, including some fields internal to the current deployment that are not needed:</p> <pre><code>k get deploy gowebapp --output yaml\n</code></pre> output<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: \"1\"\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"labels\":{\"run\":\"gowebapp\"},\"name\":\"gowebapp\",\"namespace\":\"default\"},\"spec\":{\"replicas\":1,\"selector\":{\"matchLabels\":{\"run\":\"gowebapp\"}},\"template\":{\"metadata\":{\"labels\":{\"run\":\"gowebapp\"}},\"spec\":{\"containers\":[{\"env\":[{\"name\":\"DB_PASSWORD\",\"value\":\"rootpasswd\"}],\"image\":\"ghcr.io/cloud-native-canada/k8s_setup_tools:app\",\"livenessProbe\":{\"httpGet\":{\"path\":\"/register\",\"port\":9000},\"initialDelaySeconds\":15,\"timeoutSeconds\":5},\"name\":\"gowebapp\",\"ports\":[{\"containerPort\":9000}],\"readinessProbe\":{\"httpGet\":{\"path\":\"/register\",\"port\":9000},\"initialDelaySeconds\":25,\"timeoutSeconds\":5},\"resources\":{\"limits\":{\"cpu\":\"50m\",\"memory\":\"100Mi\"},\"requests\":{\"cpu\":\"20m\",\"memory\":\"10Mi\"}}}]}}}}\n  creationTimestamp: \"2022-10-25T16:32:28Z\"\n  generation: 1\n  labels:\n    run: gowebapp\n  name: gowebapp\n  namespace: default\n  resourceVersion: \"2662\"\n  uid: c0085020-845c-468e-a95e-9bb2e908dc2b\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      run: gowebapp\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        run: gowebapp\n    spec:\n      containers:\n      - env:\n        - name: DB_PASSWORD\n          value: rootpasswd\n        image: ghcr.io/cloud-native-canada/k8s_setup_tools:app\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /register\n            port: 9000\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: gowebapp\n        ports:\n        - containerPort: 9000\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /register\n            port: 9000\n            scheme: HTTP\n          initialDelaySeconds: 25\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 50m\n            memory: 100Mi\n          requests:\n            cpu: 20m\n            memory: 10Mi\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\nstatus:\n  availableReplicas: 1\n  conditions:\n  - lastTransitionTime: \"2022-10-25T16:33:09Z\"\n    lastUpdateTime: \"2022-10-25T16:33:09Z\"\n    message: Deployment has minimum availability.\n    reason: MinimumReplicasAvailable\n    status: \"True\"\n    type: Available\n  - lastTransitionTime: \"2022-10-25T16:32:28Z\"\n    lastUpdateTime: \"2022-10-25T16:33:09Z\"\n    message: ReplicaSet \"gowebapp-5994456fcb\" has successfully progressed.\n    reason: NewReplicaSetAvailable\n    status: \"True\"\n    type: Progressing\n  observedGeneration: 1\n  readyReplicas: 1\n  replicas: 1\n  updatedReplicas: 1\n</code></pre> <p>Here, the <code>status</code> section is useless, as it relates to the current deployment. So are the <code>UID</code> or <code>creationTimestamp</code> fields. It is not recommended to store these values in your GitOps repo.</p> <p>Krew has a little plugin to trim off the un-needed parts of exported resources: Neat.</p> <p>To use Neat, just add the <code>neat</code> keyword between <code>kubectl</code> and the <code>get</code> command:</p> <p><pre><code>k neat get deploy gowebapp --output yaml\n</code></pre> output<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: \"1\"\n  labels:\n    run: gowebapp\n  name: gowebapp\n  namespace: default\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      run: gowebapp\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        run: gowebapp\n    spec:\n      containers:\n      - env:\n        - name: DB_PASSWORD\n          value: rootpasswd\n        image: ghcr.io/cloud-native-canada/k8s_setup_tools:app\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /register\n            port: 9000\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: gowebapp\n        ports:\n        - containerPort: 9000\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /register\n            port: 9000\n            scheme: HTTP\n          initialDelaySeconds: 25\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 50m\n            memory: 100Mi\n          requests:\n            cpu: 20m\n            memory: 10Mi\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      terminationGracePeriodSeconds: 30\n</code></pre></p> <p>When needed, you can then save the file for backup or later use:</p> <pre><code># k neat get deploy gowebapp --output yaml &gt; neat-app-deployment.yaml\n</code></pre>"},{"location":"kubectl_tooling/krew/#manage-kubernetes-namespaces","title":"manage Kubernetes <code>namespaces</code>","text":""},{"location":"kubectl_tooling/krew/#before","title":"Before","text":"<pre><code>kubectl get pods -n kube-system\n</code></pre> <pre><code>kubectl get pods -n kubecon\n</code></pre>"},{"location":"kubectl_tooling/krew/#after","title":"After","text":"<p>The Krew <code>ns</code> plugin is used to switch context and set it as the new default. This change is actually persisted in the <code>~/.kube/config</code> file.</p> <p>As <code>kubectl get namespaces</code>, the <code>k ns</code> command will dump the existing <code>namespaces</code> from the cluster.</p> <p>Switch to <code>kube-system</code> namespace:</p> <p><pre><code>k ns kube-system\n</code></pre> output<pre><code>Context \"kind-dev\" modified.\nActive namespace is \"kube-system\".\n</code></pre></p> <p></p> <p>Let's Deploy aplication in a new <code>namespace</code></p> <ul> <li>k8s context</li> </ul> <p><pre><code>kubectl config current-context\n</code></pre> <pre><code>kind-dev\n</code></pre></p> <ul> <li>All contexts</li> </ul> <p>We can list all Kubernetes <code>context</code> using <code>kubectl</code>:</p> <p><pre><code>kubectl config  get-contexts\n</code></pre> <pre><code>CURRENT   NAME        CLUSTER     AUTHINFO    NAMESPACE\n*         kind-dev    kind-dev    kind-dev\n</code></pre></p> <p>Before we deploy a new application, remove the application from the <code>default</code> namespace:</p> <pre><code>k delete -f ~/demo/base\n</code></pre> <p>Now create a new namespace:</p> <pre><code>k create namespace gowebapp\n</code></pre> <p>Then switch to this new namespace:</p> <pre><code>k ns gowebapp\n</code></pre> <p>Now install the GoWebApp application into this new namespace:</p> <pre><code>k apply -f ~/demo/base\n</code></pre>"},{"location":"kubectl_tooling/krew/#manage-kubernetes-context","title":"manage Kubernetes <code>context</code>","text":""},{"location":"kubectl_tooling/krew/#before_1","title":"Before","text":"<ul> <li>k8s context</li> </ul> <p><pre><code>kubectl config current-context\n</code></pre> <pre><code>kind-dev\n</code></pre></p> <ul> <li>All contexts</li> </ul> <p>We can list all Kubernetes <code>context</code> using <code>kubectl</code>:</p> <p><pre><code>kubectl config  get-contexts\n</code></pre> <pre><code>CURRENT   NAME        CLUSTER     AUTHINFO    NAMESPACE\n*         kind-dev    kind-dev    kind-dev\n</code></pre></p>"},{"location":"kubectl_tooling/krew/#after_1","title":"After","text":"<p><code>kubectl</code> is using the notion of <code>contexts</code> to define which cluster you know and which one is actuvelly being used. </p> <p>All this is defined in the <code>$HOME/.kube/config</code> file. This file can get quite large and difficult to work with.</p> <p><code>kubectl</code> has a default command to change context:</p> <p><pre><code>kubectl config get-contexts\n</code></pre> output<pre><code>CURRENT   NAME        CLUSTER     AUTHINFO    NAMESPACE\n*         kind-dev    kind-dev    kind-dev    default\n          kind-stg    kind-stg    kind-stg\n</code></pre></p> <p>Switch context to the <code>stg</code> cluster:</p> <pre><code>kubectl config use-context kind-stg\n</code></pre> <p>While this is not that long to type, we can do better with <code>ctx</code> plugin. Also, when using <code>kubecolor</code>, the current context will be highlighted:</p> <p><pre><code>k ctx\n</code></pre> output<pre><code>kind-dev\nkind-stg\n</code></pre></p> <p>You can also change context quickly by just appending the name of the target context to the same command:</p> <p><pre><code>k ctx kind-stg\n</code></pre> output<pre><code>kind-dev\nkind-stg\n</code></pre></p> <p>Finaly you can delete a context (but don't do it right now):</p> <pre><code>k ctx -d kind-stg\n</code></pre> <p></p>"},{"location":"kubectl_tooling/krew/#view-secrets","title":"View Secrets","text":"<p>Secrets in Kubernetes are an enhenced version of ConfigMaps: they are base64 encoded !</p> <p>First, create a secret:</p> <p><pre><code>k create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret -o yaml --dry-run=true\n</code></pre> output<pre><code>apiVersion: v1\ndata:\n  key1: c3VwZXJzZWNyZXQ=\n  key2: dG9wc2VjcmV0\nkind: Secret\nmetadata:\n  creationTimestamp: null\n  name: my-secret\n</code></pre></p> <p>the values are un readable because they are <code>base64</code> encoded.</p> <p>Apply that to the cluster:</p> <pre><code>k create secret generic my-secret \\\n  --from-literal=key1=supersecret \\\n  --from-literal=key2=topsecret \\\n  -o yaml --dry-run=true | k apply -n default -f -\n</code></pre> <p>It is possible to read a <code>secret</code> value by using a templated output option:</p> <p><pre><code>kubectl get secret -n default my-secret --output=go-template={{.data.key1}} | base64 --decode\n</code></pre> output<pre><code>supersecret\n</code></pre></p> <p>The Krew plugin <code>view-secret</code> does all that in one simple call:</p> <p><pre><code>k view-secret my-secret key1\n</code></pre> output<pre><code>supersecret\n</code></pre></p>"},{"location":"kubectl_tooling/krew/#next","title":"Next","text":"<p>Continue to Multi Context</p>"},{"location":"kubectl_tooling/kubecolor/","title":"kubecolor","text":"<p>As said on the project's website: <code>Colorize your kubectl output</code></p> <p>Note</p> <p>The kubecolor project, originally maintained in repo <code>https://github.com/hidetatz/kubecolor</code> has been supersedes by <code>https://github.com/kubecolor/kubecolor</code>.</p> <p>The previous maintainer wasn't answering to any bug/question and wasn't working on it anymore.</p> <p>Prune is now the maintainer of Kubecolor and is looking for other maintainers to help support it. Reach to Prune or open an issue if you want to enroll as a maintainer or report an issue.</p>"},{"location":"kubectl_tooling/kubecolor/#kubecolor-in-action","title":"Kubecolor in action","text":"<p>When using plain <code>kubectl</code>, everything is just grey (depends on your theme):</p> <p></p> <p>This is not really easy to read, and it's even worse when you have tons of outputs...</p> <p>Then enters Kubecolor:</p> <p></p> <p>This new version also support the coloring for <code>kubectl ctx</code> and <code>kubectl ns</code> commands, along highlighting the new versus old resources with a user-customized duration. Set variable <code>KUBECOLOR_OBJ_FRESH</code> to a <code>h</code>, <code>m</code> or <code>s</code> duration.</p>"},{"location":"kubectl_tooling/kubecolor/#install","title":"Install","text":"Apple MacUsing Go kubecolor install<pre><code>brew install kubecolor/tap/kubecolor\n</code></pre> kubecolor install<pre><code>VERSION=latest\ngo install -ldflags=\"-X main.Version=${VERSION}\" github.com/kubecolor/kubecolor/cmd/kubecolor@${VERSION}\n</code></pre>"},{"location":"kubectl_tooling/kubecolor/#configuration","title":"configuration","text":"<p>Usually you'll also replace all your <code>kubectl</code> commands by <code>kubecolor</code>. Edit your <code>.zshrc</code> and add:</p> ZSHBASH <pre><code>alias k=kubecolor\ncompdef kubecolor=kubectl # only needed for zsh\nexport KUBECOLOR_OBJ_FRESH=12h # highlight resources newer than 12h\n</code></pre> <p>If you are using the <code>PowerLevel10k</code> theme for ZSH, edit the line defining <code>POWERLEVEL9K_KUBECONTEXT_SHOW_ON_COMMAND</code> in file <code>~/.p10k.zsh</code> to add <code>kubecolor</code> so it behave as <code>kubectl</code>:</p> <pre><code>  typeset -g POWERLEVEL9K_KUBECONTEXT_SHOW_ON_COMMAND='kubectl|helm|kubens|kubectx|oc|istioctl|kogito|k9s|helmfile|flux|fluxctl|stern|kubeseal|skaffold|kubie|terraform|terragrunt|kubecolor'\n</code></pre> <pre><code>alias kubectl=kubecolor\nexport KUBECOLOR_OBJ_FRESH=12h # highlight resources newer than 12h\n</code></pre> <p><code>kubecolor</code> now behave the same as <code>kubectl</code>, with dynamic-prompt and completions:</p> <p></p> <p>Note</p> <p>We alias kubectl to call kubecolor. This is needed to keep all the other short <code>kubectl</code> aliases to work. In theory, <code>kubecolor</code> does not add colours when the output is not a terminal (when you pipe the command into another command). </p> <p>If you see this behaviour failing, please open an issue and force <code>kubecolor</code> to output plain text with <code>--plain</code> </p>"},{"location":"kubectl_tooling/kubecolor/#demo","title":"Demo","text":"<pre><code>export KUBECOLOR_OBJ_FRESH=12h # highlight resources newer than 12h\nk get pods -A\nk run another-test-pod --image=alpine:latest sleep 30\nk get pods\nsleep 10\nk get pods\n</code></pre>"},{"location":"kubectl_tooling/kubecolor/#next","title":"Next","text":"<p>Continue to Kubectl Extensions</p>"},{"location":"kubectl_tooling/kubectl/","title":"Kubectl","text":"<p><code>kubectl</code> may not be expected to appear in the <code>tooling</code> section as it's, well, the basic tool.</p> <p>But maybe there's some commands and tricks that are worth mentioning to use it at its full potential ? </p> <p>So here are some cool usage of <code>kubectl</code>:</p>"},{"location":"kubectl_tooling/kubectl/#create-a-vanilla-resource","title":"Create a vanilla resource","text":"<pre><code>k create deployment sample_app \\\n  --image=alpine \\\n  --dry-run=client \\\n  --output yaml &gt; sample_deployment.yaml\n</code></pre> <p>Which will result in the following deployment to be saved:</p> sample_app.yaml<pre><code>cat &gt; /tmp/toto &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: sample_app\n  name: sample_app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sample_app\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: sample_app\n    spec:\n      containers:\n      - image: alpine\n        name: alpine\n        resources: {}\nstatus: {}\nEOF\n</code></pre> <p>Then it's easy to edit the yaml file and fill the voides.</p>"},{"location":"kubectl_tooling/kubectl/#edit-a-resource-in-the-cluster","title":"Edit a resource in the cluster","text":"<p>The command <code>kubectl edit</code> can be used to edit a resource directly in the cluster. </p> <p>While it's better to manage your resources <code>as code</code>, versioned in a <code>git</code> repo (#Gitops), it's sometimes faster to directly edit the resource. Such case is changing the <code>replicas</code> value of a deployment or updating a label, or removing a <code>Finalizer</code> so the resource can be deleted. </p> <p>Let's edit our <code>simple-deployment</code> and scale it to 3:</p> <pre><code>kubectl edit deployment gowebapp\n</code></pre> <p>Then go to the line with <code>replicas: 1</code> and change it to <code>replicas: 2</code>.</p> <p>By default, <code>kubectl</code> is using <code>vi</code> as an editor. This can be changed by setting a value to the <code>KUBE_EDITOR</code> env variable. You can add it to your <code>.zshrc</code> or <code>.bashrc</code> to make it permanent.</p> <p>If you're using <code>vi</code>, highlight the number <code>1</code>, type <code>r</code> then <code>2</code> then <code>:wq</code></p> <p>If you made a mistake, just exit without saving using <code>:q!</code></p> <p>If you prefer to change the editor, set the <code>KUBE_EDITOR</code> variable:</p> MacOsXLinuxWindows <p>Use SublimeText<pre><code>export KUBE_EDITOR=\"/Applications/Sublime.app/Contents/SharedSupport/bin/subl -w\"\n</code></pre> Use VsCode<pre><code>export KUBE_EDITOR=\"code --wait\"\n</code></pre></p> <pre><code>export KUBE_EDITOR=\"ed\"\n</code></pre> <pre><code>KUBE_EDITOR=code -w\n</code></pre> <p>Of course, there is a <code>kubectl</code> command to change the number of <code>replicas</code>: </p> <p><pre><code>kubectl scale --replicas=2 deployment/gowebapp\nkubectl get pods\n</code></pre> output<pre><code>NAME                                 READY   STATUS             RESTARTS       AGE\ngowebapp-57d9ccc7f8-2m97j            0/1     CrashLoopBackOff   7 (73s ago)    12m\ngowebapp-57d9ccc7f8-bm8jp            0/1     CrashLoopBackOff   5 (119s ago)   4m48s\n</code></pre></p>"},{"location":"kubectl_tooling/kubectl/#im-still-bored-to-use-kubectl","title":"I'm still bored to use <code>kubectl</code>","text":"<p>And this is right. We added a ton of stuff in our shell to be faster. Let's revisit what we did previously:</p> <ul> <li> <p>List the pods     <pre><code>kgp\n</code></pre> output<pre><code>NAME                                 READY   STATUS             RESTARTS       AGE\nsimple-deployment-57d9ccc7f8-2m97j   0/1     CrashLoopBackOff   7 (73s ago)    12m\nsimple-deployment-57d9ccc7f8-bm8jp   0/1     CrashLoopBackOff   5 (119s ago)   4m48s\nsimple-deployment-57d9ccc7f8-bqnxs   0/1     CrashLoopBackOff   9 (5m ago)     26m\nsimple-pod                           1/1     Running            0              26m\n</code></pre></p> </li> <li> <p>change the replicas of the deployment     <pre><code>ksd --replicas=2 simple-deployment\n</code></pre> output<pre><code>deployment.apps/simple-deployment scaled\n</code></pre></p> </li> </ul>"},{"location":"kubectl_tooling/kubectl/#next","title":"Next","text":"<p>Continue to KubeColor</p>"},{"location":"kubectl_tooling/kubie/","title":"Kubie","text":"<p>Kubie offers context switching, namespace switching and prompt modification in a way that makes each shell independent from others.</p> <p>This is an alternative to <code>k ctx</code> and <code>k ns</code> in a way where instead os switching your <code>context</code> back and forth and use the same <code>context</code> in all your <code>terminals</code> (all your <code>shells</code>), you pin one <code>context</code> per <code>terminal</code>.</p> <p>Under the hood, it automates the use of the <code>KUBECONFIG</code> env variable to allow each shell to be configured differently.</p> <p>Note</p> <p>The <code>KUBECONFIG</code> variable, if set, will tell <code>kubectl</code> to use a specific <code>config</code> file instead of the default <code>~/.kube/config</code> file.</p> <p>Warning</p> <p>It can be dangerous to use <code>kubie</code> as you may just enter a command in the wrong shell. It requires a little bit more of attention. Same thing as running <code>rm -rf *</code> and using <code>ssh</code> toward a production server.</p> <p>It is easier to control when all your shells targets only one cluster and you need to use the \u2013context parameter to switch.</p> <p>At least, if you use Kubie, ensure your shell\u2019s prompt is clearly displaying the cluster/namespace you\u2019re in !</p> <p>Then, it works almost the same as <code>ctx</code>/<code>ns</code>, except the selection is only for the current shell.</p> <p>The cool feature is that you can execute a command in some (or all) of the contexts base on a regexp\u2026 it can be handy sometimes</p>"},{"location":"kubectl_tooling/kubie/#install","title":"Install","text":"Apple MacOsXLinuxWindows <pre><code>brew install kubie\n</code></pre> <p>You can download a binary for Linux or OS X on the GitHub releases page.</p> <p>Use <code>curl</code> or <code>wget</code> to download it. Don't forget to <code>chmod +x</code> the file:</p> <pre><code>wget https://github.com/sbstp/kubie/releases/download/v0.19.0/kubie-linux-amd64\nchmod 755 kubie-linux-amd64\nsudo mv kubie-linux-amd64 /usr/local/bin/kubie\n</code></pre> <p>TODO</p>"},{"location":"kubectl_tooling/kubie/#usage","title":"Usage","text":"<ul> <li> <p>display a selectable menu of contexts     <pre><code>kubie ctx\n</code></pre></p> </li> <li> <p>display a selectable menu of namespaces     <pre><code>kubie ns\n</code></pre></p> </li> <li> <p>execute a command in all contexts matched by the wildcard and in the given namespace     <pre><code>kubie exec &lt;wildcard&gt; &lt;namespace&gt; &lt;cmd&gt; &lt;args&gt;\n</code></pre></p> </li> </ul>"},{"location":"kubectl_tooling/kubie/#multi-context-demo","title":"Multi-Context Demo","text":"<p>Before we start using <code>kubie</code>, we have two contexts, one for the <code>kind-demo</code> and one for the <code>kind-demo2</code> cluster. Let's switch to the <code>kind-demo</code> context:</p> <p><pre><code>k ctx kind-demo\nk ctx\n</code></pre> output<pre><code>kind-demo\nkind-demo2\n</code></pre></p> <p>Open a second shell/terminal, and run the same command but switch to the <code>kind-demo2</code> context:</p> <p><pre><code>k ctx kind-demo2\nk ctx\n</code></pre> output<pre><code>kind-demo\nkind-demo2\n</code></pre></p> <p>Go back in the previous shell, and check which context you're in:</p> <p><pre><code>k ctx\n</code></pre> output<pre><code>kind-demo\nkind-demo2\n</code></pre></p> <p>The default context is still set to <code>kind-demo2</code>. This is because at this moment the <code>context</code> is global and set in the <code>~/.kube.config</code> file. We can check that using grep:</p> <p><pre><code>cat  ~/.kube/config | grep current-context\n</code></pre> output<pre><code>current-context: kind-demo2\n</code></pre></p> <p>In the first shell, use <code>kubie</code> to switch to the <code>kind-demo2</code> context:</p> <p><pre><code>kubie ctx kind-demo2\n\nk ctx\n</code></pre> output<pre><code>kind-demo2\n</code></pre></p> <p>Now only one <code>context</code> is displayed. This is because <code>kubie</code> re-configured your context and statically pinned one cluster.</p> <p>Under the hood, <code>kubie</code> used the <code>KUBECONFIG</code> env variable and set it to use a new config file:</p> <p><pre><code>env | grep KUBE\n</code></pre> output<pre><code>KUBECONFIG=/var/folders/mt/b1zldbxs4wnf78m2p0mv444h0000gn/T/kubie-configMawRW2.yaml\nKUBIE_KUBECONFIG=/var/folders/mt/b1zldbxs4wnf78m2p0mv444h0000gn/T/kubie-configMawRW2.yaml\n</code></pre> If we look into this file, it's a new file similar to the <code>~/.kube/config</code> file but with only one cluster in it:</p> <pre><code>---\nclusters:\n  - name: kind-demo2\n    cluster:\n      certificate-authority: /Users/prune/.kind-demo2/ca.crt\n      extensions:\n        - extension:\n            last-update: \"Thu, 20 Oct 2022 18:17:15 EDT\"\n            provider: kind.sigs.k8s.io\n            version: v1.27.1\n          name: cluster_info\n      server: \"https://192.168.64.2:8443\"\nusers:\n  - name: kind-demo2\n    user:\n      client-certificate: /Users/prune/.kind-demo2/profiles/kind-demo2/client.crt\n      client-key: /Users/prune/.kind-demo2/profiles/kind-demo2/client.key\ncontexts:\n  - name: kind-demo2\n    context:\n      cluster: kind-demo2\n      namespace: ~\n      user: kind-demo2\ncurrent-context: kind-demo2\napiVersion: v1\nkind: Config\n</code></pre> <p>In the second shell/terminal, use <code>kubie</code> to switch to the <code>kind-demo</code> context:</p> <p><pre><code>kubie ctx kind-demo\n\nk ctx\n</code></pre> output<pre><code>kind-demo\n</code></pre></p> <p>This shell is in the <code>kind-demo</code> context. Go back to the first shell and check:</p> <p><pre><code>k ctx\n</code></pre> output<pre><code>minikube\n</code></pre></p>"},{"location":"kubectl_tooling/kubie/#i-dont-know-which-context-im-using","title":"I don't know which context i'm using","text":"<p>Having two different context in two co-located shells is really dangerous. It's better to identify which cluster is selected before typing a <code>kubectl</code> command which could change some resources.</p> <p>To solve this, the <code>k ctx</code> command is helpful and will highlight the current context. <code>kubie info ctx</code> will do the same.</p> <p>thanks to Oh My ZSH! and the Powerline10k theme, the prompt is enhanced and now displays a ton of useful infos.</p> <p>The default prompt is adaptative and depends on what command are typed in which folder. Here are some example:</p>"},{"location":"kubectl_tooling/kubie/#current-folder","title":"Current folder","text":"<p>The left side of the prompt, in blue, shows the current folder.</p> <p></p> <p>When the folder is a <code>git</code> repo (contains a .git folder), the prompt is extended to show the branch name:</p> <p></p> <p>Switching to the <code>docker</code> branch changes the prompt:</p> <p></p> <p>If a file is modified, the prompt color switch to yellow:</p> <p></p>"},{"location":"kubectl_tooling/kubie/#last-command-status","title":"Last command status","text":"<p>The right prompt shows the date by default.</p> <p>When you run a command, a <code>status</code> extension is added. It is green in case of success and red in case of error. </p> <p>If the command is composed of multiple commands with pipes, each command status will be reported:</p> <p> </p> <p>When the command is failing with a delay, like a connection timeout, the duration is also added:</p> <p></p>"},{"location":"kubectl_tooling/kubie/#kubernetes-status","title":"Kubernetes status","text":"<p>Some applications also benefit from a specific <code>dynamic prompt</code>, triggered by the name of the command. </p> <p>If your shell is well configured, typing <code>k</code> or <code>kubectl</code> should add a new section to the right prompt that displays the current <code>context</code>:</p> <p></p> <p>When the selected namespace in NOT <code>default</code>, it is also added to the <code>context</code>:</p> <p></p> <p>Refer to Dynamic Prompt section to get more infos.</p>"},{"location":"kubectl_tooling/kubie/#next","title":"Next","text":"<p>Continue to explore the cool commands to look at logs</p>"},{"location":"kubectl_tooling/stern/","title":"Stern","text":"<p>Stern allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging.</p>"},{"location":"kubectl_tooling/stern/#install","title":"Install","text":"<p>Warning</p> <p>Note that the original Stern repo was forked at https://github.com/stern/stern as the other one is not maintained anymore.</p> Apple MacOsXGoKrew <pre><code>brew install stern\n</code></pre> <pre><code>go install github.com/stern/stern@latest\n</code></pre> <pre><code>kubectl krew install stern\n</code></pre>"},{"location":"kubectl_tooling/stern/#dumping-the-logs-with-kubectl","title":"Dumping the logs with kubectl","text":"<p>First let's add a new <code>deployment</code> which generate some logs:</p> <pre><code>cat &gt; multi-deployment.yaml &lt;&lt; EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: multi-deployment\n  name: multi-deployment\n  namespace: default\nspec:\n  replicas: 2\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: multi-deployment\n  template:\n    metadata:\n      labels:\n        app: multi-deployment\n    spec:\n      containers:\n      - image: alpine:latest\n        imagePullPolicy: Always\n        name: first\n        command: [\"/bin/sh\", \"-c\"]\n        args:\n          - echo \"starting first container\";\n            tail -f /dev/null\n      - image: alpine:latest\n        imagePullPolicy: Always\n        name: second\n        command: [\"/bin/sh\", \"-c\"]\n        args:\n          - echo \"starting second container\";\n            tail -f /dev/null\nEOF\n\nk apply -n default -f multi-deployment.yaml\n</code></pre> <p>Two new pods are created, each with two containers:</p> <p><pre><code>kgp -l app=multi-deployment\n</code></pre> output<pre><code>NAME                                READY   STATUS    RESTARTS   AGE\nmulti-deployment-6f4fd4f8b8-lg2b8   2/2     Running   0          4m52s\nmulti-deployment-6f4fd4f8b8-vnqbx   2/2     Running   0          4m49s\n</code></pre></p> <p>Here is a command to access the logs:</p> <p><pre><code>k logs -n default -l app=multi-deployment\n</code></pre> output<pre><code>Defaulted container \"first\" out of: first, second\nDefaulted container \"first\" out of: first, second\nstarting first container\nstarting first container\n</code></pre></p> <p><code>kubectl</code> is complaining that our pods have multiple <code>containers</code>, and is only dumping the logs from the first one.</p> <p>Using the <code>--all-containers</code> argument allows to dump them all:</p> <p><pre><code>k logs -n default -l app=multi-deployment --all-containers\n</code></pre> output<pre><code>starting first container\nstarting second container\nstarting first container\nstarting second container\n</code></pre></p> <p>As you can see, there's multiple things to complain about:</p> <ul> <li>it's not easy to read</li> <li>it is based on <code>label</code> selectors</li> <li>you have to select one container if there are many in the pod</li> <li>when dumping all containers, we have no idea which one logged what</li> <li>we have to select a namespace or use the current one</li> </ul> <p>Warning</p> <p>By default, <code>kubectl logs</code> displays the logs and stop. The <code>--follow</code> (<code>-f</code>) option must be set to <code>tail</code> the logs .</p>"},{"location":"kubectl_tooling/stern/#using-stern","title":"Using Stern","text":"<p>Stern is a small app that extend how logs are dumped, adding the name of the pod, the container inside the pod, and using colors to differentiate which is which. It also add an easier way to select the pods to display.</p> <p><code>stern</code> does not have to use <code>label</code> selectors (but you can). By default it use the argument as a regular-expression to search for matching pods. Ex:</p> <p><pre><code>stern multi\n</code></pre> output<pre><code>+ multi-deployment-6f4fd4f8b8-vnqbx \u203a first\n+ multi-deployment-6f4fd4f8b8-vnqbx \u203a second\n+ multi-deployment-6f4fd4f8b8-lg2b8 \u203a second\n+ multi-deployment-6f4fd4f8b8-lg2b8 \u203a first\nmulti-deployment-6f4fd4f8b8-lg2b8 second starting second container\nmulti-deployment-6f4fd4f8b8-vnqbx second starting second container\nmulti-deployment-6f4fd4f8b8-lg2b8 first starting first container\nmulti-deployment-6f4fd4f8b8-vnqbx first starting first container\n</code></pre></p> <p>Here's a detailed explanation:</p> <p></p> <p>Stern is really versatile, and here are some command examples, based on the previous <code>multi-deployment</code> logs:</p> <ul> <li> <p>search pods per labels (as with <code>k logs</code>)</p> <p><pre><code>stern -n default -l app=multi-deployment\n</code></pre> output<pre><code>+ multi-deployment-6f4fd4f8b8-vnqbx \u203a first\n+ multi-deployment-6f4fd4f8b8-vnqbx \u203a second\n+ multi-deployment-6f4fd4f8b8-lg2b8 \u203a second\n+ multi-deployment-6f4fd4f8b8-lg2b8 \u203a first\nmulti-deployment-6f4fd4f8b8-lg2b8 second starting second container\nmulti-deployment-6f4fd4f8b8-vnqbx second starting second container\nmulti-deployment-6f4fd4f8b8-lg2b8 first starting first container\nmulti-deployment-6f4fd4f8b8-vnqbx first starting first container\n</code></pre> - search pods in all namespaces</p> <p><pre><code>k ns kube-system\nstern multi --all-namespaces\n</code></pre> output<pre><code>+ multi-deployment-6f4fd4f8b8-vnqbx \u203a first\n+ multi-deployment-6f4fd4f8b8-vnqbx \u203a second\n+ multi-deployment-6f4fd4f8b8-lg2b8 \u203a second\n+ multi-deployment-6f4fd4f8b8-lg2b8 \u203a first\nmulti-deployment-6f4fd4f8b8-lg2b8 second starting second container\nmulti-deployment-6f4fd4f8b8-vnqbx second starting second container\nmulti-deployment-6f4fd4f8b8-lg2b8 first starting first container\nmulti-deployment-6f4fd4f8b8-vnqbx first starting first container\n</code></pre> - Exclude some logs with a matching pattern</p> <p><pre><code># exclude logs with [INFO]\nstern -ndefault multi --exclude \"second\"\n</code></pre> output<pre><code>+ multi-deployment-6f4fd4f8b8-zswfr \u203a first\n+ multi-deployment-6f4fd4f8b8-zswfr \u203a second\n+ multi-deployment-6f4fd4f8b8-lg2b8 \u203a second\n+ multi-deployment-6f4fd4f8b8-lg2b8 \u203a first\nmulti-deployment-6f4fd4f8b8-lg2b8 first starting first container\nmulti-deployment-6f4fd4f8b8-zswfr first starting first container\n</code></pre> - Tail only the latest logs (drop old logs)</p> <p><pre><code>stern -n kube-system coredns --tail 1\n</code></pre> output<pre><code>+ coredns-6d4b75cb6d-plpj9 \u203a coredns\n+ coredns-6d4b75cb6d-cw498 \u203a coredns\ncoredns-6d4b75cb6d-cw498 coredns linux/amd64, go1.17.1, 13a9191\ncoredns-6d4b75cb6d-plpj9 coredns linux/amd64, go1.17.1, 13a9191\n</code></pre> - Tail logs in Json</p> <p>This is great as you can use <code>jq</code> to pretty-print the logs, or apply some complex <code>json</code> filtering:</p> <p>Note</p> <p>The real log message is in the <code>message</code> field, and is encoded. In the case your logs are already JSON, they will be double-encoded, and not pure JSON.</p> <p>The latest version of Stern (<code>1.22.0</code> or newer) inclused two other <code>--ouptput</code> modes:  - <code>extjson</code>     This is an extended JSON output, used when your logs are already JSON. In this case, they will not be double-encoded. - <code>ppextjson</code>     This is the same as above but keeping the stern colors to identify pods and adding pretty-print indentation so it not necessary to use <code>jq</code></p> <p>Here is an updated deployment that logs in json:</p> <pre><code>cat &gt; multi-deployment.yaml &lt;&lt; EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: multi-deployment\n  name: multi-deployment\n  namespace: default\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: multi-deployment\n  template:\n    metadata:\n      labels:\n        app: multi-deployment\n    spec:\n      containers:\n      - image: alpine:latest\n        imagePullPolicy: Always\n        name: first\n        command: [\"/bin/sh\", \"-c\"]\n        args:\n          - echo '{\"level\":\"info\",\"message\":\"starting\",\"container\":\"first\"}';\n            sleep 5;\n            echo '{\"level\":\"warn\",\"message\":\"something is not quite right\",\"container\":\"first\"}';\n            tail -f /dev/null\n      - image: alpine:latest\n        imagePullPolicy: Always\n        name: second\n        command: [\"/bin/sh\", \"-c\"]\n        args:\n          - echo '{\"level\":\"info\",\"message\":\"starting\",\"container\":\"second\"}';\n            sleep 2;\n            echo '{\"level\":\"warn\",\"message\":\"Redrum. Redrum. REDRUM!\",\"container\":\"second\"}';\n            tail -f /dev/null\nEOF\n\nk apply -n default -f multi-deployment.yaml\n</code></pre> <p>dumping the logs in <code>json</code> format still encodes the real logs as a <code>message</code>:</p> <p><pre><code>stern multi -o json | jq '.'\n</code></pre> output<pre><code>+ multi-deployment-6f9fb8c49d-l4tzx \u203a second\n+ multi-deployment-6f9fb8c49d-l4tzx \u203a first\n{\n  \"message\": \"{\\\"level\\\":\\\"info\\\",\\\"message\\\":\\\"starting\\\",\\\"container\\\":\\\"second\\\"}\",\n  \"nodeName\": \"demo-worker\",\n  \"namespace\": \"default\",\n  \"podName\": \"multi-deployment-6f9fb8c49d-l4tzx\",\n  \"containerName\": \"second\"\n}\n{\n  \"message\": \"{\\\"level\\\":\\\"warn\\\",\\\"message\\\":\\\"Redrum. Redrum. REDRUM!\\\",\\\"container\\\":\\\"second\\\"}\",\n  \"nodeName\": \"demo-worker\",\n  \"namespace\": \"default\",\n  \"podName\": \"multi-deployment-6f9fb8c49d-l4tzx\",\n  \"containerName\": \"second\"\n}\n{\n  \"message\": \"{\\\"level\\\":\\\"info\\\",\\\"message\\\":\\\"starting\\\",\\\"container\\\":\\\"first\\\"}\",\n  \"nodeName\": \"demo-worker\",\n  \"namespace\": \"default\",\n  \"podName\": \"multi-deployment-6f9fb8c49d-l4tzx\",\n  \"containerName\": \"first\"\n}\n{\n  \"message\": \"{\\\"level\\\":\\\"warn\\\",\\\"message\\\":\\\"something is not quite right\\\",\\\"container\\\":\\\"first\\\"}\",\n  \"nodeName\": \"demo-worker\",\n  \"namespace\": \"default\",\n  \"podName\": \"multi-deployment-6f9fb8c49d-l4tzx\",\n  \"containerName\": \"first\"\n}\n</code></pre></p> <p>Stern can now dump real <code>json</code> when you already have <code>json</code> logs by using the <code>extjson</code> output:</p> <p><pre><code>stern multi -o extjson | jq '.'\n</code></pre> output<pre><code>+ multi-deployment-6f9fb8c49d-l4tzx \u203a second\n+ multi-deployment-6f9fb8c49d-l4tzx \u203a first\n{\n  \"pod\": \"multi-deployment-6f9fb8c49d-l4tzx\",\n  \"container\": \"first\",\n  \"message\": {\n    \"level\": \"info\",\n    \"message\": \"starting\",\n    \"container\": \"first\"\n  }\n}\n{\n  \"pod\": \"multi-deployment-6f9fb8c49d-l4tzx\",\n  \"container\": \"first\",\n  \"message\": {\n    \"level\": \"warn\",\n    \"message\": \"something is not quite right\",\n    \"container\": \"first\"\n  }\n}\n{\n  \"pod\": \"multi-deployment-6f9fb8c49d-l4tzx\",\n  \"container\": \"second\",\n  \"message\": {\n    \"level\": \"info\",\n    \"message\": \"starting\",\n    \"container\": \"second\"\n  }\n}\n{\n  \"pod\": \"multi-deployment-6f9fb8c49d-l4tzx\",\n  \"container\": \"second\",\n  \"message\": {\n    \"level\": \"warn\",\n    \"message\": \"Redrum. Redrum. REDRUM!\",\n    \"container\": \"second\"\n  }\n}\n</code></pre></p> <p>This is far more readable.</p> <p>The last <code>stern</code> option is the <code>ppextjson</code> json, which will pretty-print and colorize the output so it is not needed to use <code>jq</code>:</p> <p><pre><code>stern multi -o ppextjson | jq '.'\n</code></pre> output<pre><code>+ multi-deployment-6f9fb8c49d-l4tzx \u203a second\n+ multi-deployment-6f9fb8c49d-l4tzx \u203a first\n{\n  \"pod\": \"multi-deployment-6f9fb8c49d-l4tzx\",\n  \"container\": \"second\",\n  \"message\": {\"level\":\"info\",\"message\":\"starting\",\"container\":\"second\"}\n}\n{\n  \"pod\": \"multi-deployment-6f9fb8c49d-l4tzx\",\n  \"container\": \"second\",\n  \"message\": {\"level\":\"warn\",\"message\":\"Redrum. Redrum. REDRUM!\",\"container\":\"second\"}\n}\n{\n  \"pod\": \"multi-deployment-6f9fb8c49d-l4tzx\",\n  \"container\": \"first\",\n  \"message\": {\"level\":\"info\",\"message\":\"starting\",\"container\":\"first\"}\n}\n{\n  \"pod\": \"multi-deployment-6f9fb8c49d-l4tzx\",\n  \"container\": \"first\",\n  \"message\": {\"level\":\"warn\",\"message\":\"something is not quite right\",\"container\":\"first\"}\n}\n</code></pre></p> </li> </ul>"},{"location":"kubectl_tooling/stern/#next","title":"Next","text":"<p>Cloud Kubernetes is a thing now. Learn how to install and configure it in next chapter.</p>"},{"location":"kustomize/kustomize/","title":"Kustomize","text":"<p>Kustomize is a Kubernetes native configuration management based on Overlays.</p> <ul> <li>Bundled with kubectl, but not all the features are available</li> <li>It's better install the full version if you use it intensively (like plugins)</li> <li>It only output rendered YAML, you have to apply it with another command</li> <li>Can be used on top of Helm</li> </ul>"},{"location":"kustomize/kustomize/#install","title":"Install","text":"<p>Check the official site.</p> Max OS XAll OSGolang install <pre><code>brew install kustomize\n</code></pre> <pre><code>curl -s \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\"  | bash\n</code></pre> <pre><code>GOBIN=$(pwd)/ GO111MODULE=on go install sigs.k8s.io/kustomize/kustomize/v4@latest\n</code></pre>"},{"location":"kustomize/kustomize/#setup-your-project-for-kustomize","title":"Setup your project for Kustomize","text":"<p>Kustomize requires a <code>kustomization.yaml</code> file along other <code>yaml</code> file.</p> <p>For this DEMO, we created a <code>demo</code> folder and a <code>base</code> folder inside of it, where we created the yaml files to deploy the application.</p> <p>Now add a <code>kustomization.yaml</code> file along the other <code>yaml</code>s:</p> <pre><code>cat &gt; kustomization.yaml &lt;&lt; EOF\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nmetadata:\n  name: gowebapp\n\nresources:\n- app-deployment.yaml\n- app-service.yaml\n- mysql-deployment.yaml\n- mysql-secret.yaml\n- mysql-service.yaml\nEOF\n</code></pre> <p>It is easy to generate the final result using the kustomize` command:</p> <p><pre><code>kustomize build\n# Same as kubectl kustomize base\n</code></pre> output<pre><code>apiVersion: v1\ndata:\n  password: cm9vdHBhc3N3ZA==\nkind: Secret\nmetadata:\n  name: mysql\ntype: Opaque\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    run: gowebapp\n  name: gowebapp\nspec:\n  ports:\n  - port: 9000\n    targetPort: 9000\n  selector:\n    run: gowebapp\n  type: LoadBalancer\n---\napiVersion: v1\nkind: Service\n...\n</code></pre></p>"},{"location":"kustomize/kustomize/#add-an-overlay-for-the-dev-environment","title":"Add an <code>overlay</code> for the <code>dev</code> environment","text":"<p>Overlays are variations on the original yaml that you apply \"on top\" of the base. Configuration of overlays are also done in a <code>kustomization.yaml</code> file in a different folder.</p> <p>First we have to create a new folder for overlays:</p> <pre><code>cd ..\nmkdir -p overlays/dev overlays/stg\n</code></pre> <p>Now the folder structure should be like:</p> <pre><code>.\n\u251c\u2500\u2500 base\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 app-deployment.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 app-service.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mysql-deployment.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mysql-secret.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 mysql-service.yaml\n\u2514\u2500\u2500 overlays\n    \u251c\u2500\u2500 dev\n    \u2514\u2500\u2500 stg\n</code></pre> <p>Let's create an Overlay to ensure the <code>dev</code> version is deploying inside the <code>dev</code> namespace:</p> <pre><code>cat &lt;&lt;EOF &gt;overlays/dev/kustomization.yaml\nresources:\n- ./../../base\n\nnamespace: dev\nnamePrefix: dev-\nEOF\n</code></pre> <p>We can generate the resulting yaml for the <code>dev</code> cluster:</p> <p><pre><code>kustomize build overlays/dev\n</code></pre> output<pre><code>...\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: gowebapp-mysql\n    tier: backend\n  name: dev-gowebapp-mysql\n  namespace: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: gowebap\n...\n</code></pre></p> <p>Here, the <code>metadata.name</code> and <code>metadata.namespace</code> were updated with our new values.</p> <p>But the <code>dev</code> namespace actually does not exist in our cluster. </p> <p>We can have Kustomize create it by adding a new <code>namespace</code> resource:</p> <pre><code>cat &lt;&lt;EOF &gt;overlays/dev/namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\nEOF\n</code></pre> <p>Then we include this new resource to our kustomization:</p> <pre><code>cat &lt;&lt;EOF &gt;overlays/dev/kustomization.yaml\nresources:\n- ./../../base\n- namespace.yaml\n\nnamespace: dev\nnamePrefix: dev-\nEOF\n</code></pre> <p>When re-running the kustomize command, we now also create the namespace:</p> <p><pre><code>kustomize build overlays/dev\n</code></pre> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n---\napiVersion: v1\ndata:\n...\n</code></pre></p> <p>All this is getting better, but it's common practices to add some <code>labels</code> to the resources to describe their role, owner or environment.</p> <p>We can fix that by overriding the labels with a <code>commonLabels</code> keyword :</p> <pre><code>cat &lt;&lt;EOF &gt;overlays/dev/kustomization.yaml\nresources:\n- ./../../base\n- namespace.yaml\n\nnamespace: dev\nnamePrefix: dev-\n\ncommonLabels:\n  app: gowebapp\n  owner: prune\n  version: v1\n  environment: dev\nEOF\n</code></pre> <p>As expected we have new labels:</p> <p><pre><code>kustomize build overlays/dev\n</code></pre> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    app: gowebapp\n    environment: dev\n    owner: prune\n    version: v1\n  name: dev\n---\napiVersion: v1\ndata:\n  password: cm9vdHBhc3N3ZA==\nkind: Secret\nmetadata:\n  labels:\n    app: gowebapp\n    environment: dev\n    owner: prune\n    version: v1\n  name: dev-mysql\n  namespace: dev\ntype: Opaque\n...\n</code></pre></p>"},{"location":"kustomize/kustomize/#add-an-overlay-for-staging","title":"add an overlay for staging","text":"<p>We can do the same for staging :</p> <pre><code>cat &lt;&lt;EOF &gt;overlays/stg/kustomization.yaml\nresources:\n- ./../../base\n- namespace.yaml\n\nnamespace: stg\nnamePrefix: stg-\n\ncommonLabels:\n  app: gowebapp\n  owner: prune\n  version: v1\n  environment: stg\nEOF\n\ncat &lt;&lt;EOF &gt;overlays/stg/namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: stg\nEOF\n</code></pre>"},{"location":"local_cluster/kind/","title":"Kind","text":"<p>Important</p> <p>If you already have Docker-Desktop (Mac, Windows) or Docker Engine (Linux) installed and running, skip this step for the demo and go directly to Deploying applications with kubectl</p> <p>Kind is an amazing tool for running test clusters locally as it runs in a container which makes it lightweight and easy to run throw-away clusters for testing purposes.</p> <ul> <li>It can be used to deploy Local K8s cluster or for CI</li> <li>Support ingress / LB (with some tuning)</li> <li>Support deployment of multiple clusters / versions</li> <li>Supports deployment of single or multi node clusters</li> </ul> <p>For more information, check out https://kind.sigs.k8s.io/.</p>"},{"location":"local_cluster/kind/#install","title":"Install","text":"Apple Mac OsXLinuxWindows OsX Install<pre><code>brew install kind\n</code></pre> Linux Install<pre><code>wget https://github.com/kubernetes-sigs/kind/releases/download/v0.15.0/kind-linux-amd64\nchmod 755 kind-linux-amd64\n\nmv kind-linux-amd64 /usr/local/bin/kind\n</code></pre> Windows Install<pre><code># TODO\n</code></pre>"},{"location":"local_cluster/kind/#usage","title":"Usage","text":"<p>To create a K8s cluster with <code>Kind</code> use the command:</p> <pre><code>kind create cluster --help\n</code></pre>"},{"location":"local_cluster/kind/#create-a-first-kind-cluster-dev","title":"Create a first kind cluster <code>dev</code>","text":"<p>In this guide we will run 2 clusters side by side <code>dev</code> and <code>stg</code>.</p> <p>In order to have consitency over <code>kind</code> cluster configuration, create cluster by specifing a config file. and settings such <code>k8s version</code> and etc,</p> <p>Here's the config yaml file:</p> kind-dev.yaml<pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nkubeadmConfigPatches:\n- |\n  apiVersion: kubeadm.k8s.io/v1beta2\n  kind: ClusterConfiguration\n  metadata:\n    name: config\n  apiServer:\n    extraArgs:\n      \"service-account-issuer\": \"kubernetes.default.svc\"\n      \"service-account-signing-key-file\": \"/etc/kubernetes/pki/sa.key\"\nnetworking:\n  # the default CNI will not be installed if you enable it, usefull to install Cilium !\n  disableDefaultCNI: false\nnodes:\n- role: control-plane\n  image: kindest/node:v1.24.4@sha256:adfaebada924a26c2c9308edd53c6e33b3d4e453782c0063dc0028bdebaddf98\n- role: worker\n  image: kindest/node:v1.24.4@sha256:adfaebada924a26c2c9308edd53c6e33b3d4e453782c0063dc0028bdebaddf98\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 3080\n    listenAddress: \"0.0.0.0\"\n  - containerPort: 443\n    hostPort: 3443\n    listenAddress: \"0.0.0.0\"\n</code></pre> <p>Then create the cluster from this file:</p> <pre><code>cd ~/demo/\n</code></pre> <p><pre><code>cat &gt; kind-dev.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nkubeadmConfigPatches:\n- |\n  apiVersion: kubeadm.k8s.io/v1beta2\n  kind: ClusterConfiguration\n  metadata:\n    name: config\n  apiServer:\n    extraArgs:\n      \"service-account-issuer\": \"kubernetes.default.svc\"\n      \"service-account-signing-key-file\": \"/etc/kubernetes/pki/sa.key\"\nnetworking:\n  # the default CNI will not be installed if you enable it, usefull to install Cilium !\n  disableDefaultCNI: false\nnodes:\n- role: control-plane\n  image: kindest/node:v1.24.4@sha256:adfaebada924a26c2c9308edd53c6e33b3d4e453782c0063dc0028bdebaddf98\n- role: worker\n  image: kindest/node:v1.24.4@sha256:adfaebada924a26c2c9308edd53c6e33b3d4e453782c0063dc0028bdebaddf98\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 3080\n    listenAddress: \"0.0.0.0\"\n  - containerPort: 443\n    hostPort: 3443\n    listenAddress: \"0.0.0.0\"\nEOF\n</code></pre> <pre><code>kind create cluster --name=dev --config kind-dev.yaml -v9 --retain\n</code></pre></p> <p>Here's the regular logs when starting a Kind cluster:</p> <pre><code>enabling experimental podman provider\nCreating cluster \"dev\" ...\n \u2713 Ensuring node image (kindest/node:v1.24.4) \ud83d\uddbc\n \u2713 Preparing nodes \ud83d\udce6 \ud83d\udce6\n \u2713 Writing configuration \ud83d\udcdc\n \u2713 Starting control-plane \ud83d\udd79\ufe0f\n \u2713 Installing CNI \ud83d\udd0c\n \u2713 Installing StorageClass \ud83d\udcbe\n \u2713 Joining worker nodes \ud83d\ude9c\nSet kubectl context to \"kind-dev\"\n</code></pre> <p>You can check that everything is working. Each K8s node is actually a running <code>container</code>:</p> <p><pre><code>podman ps\n</code></pre> <pre><code>CONTAINER ID  IMAGE                                                                                           COMMAND     CREATED        STATUS            PORTS                                        NAMES\n6993dbdbf82b  docker.io/kindest/node@sha256:adfaebada924a26c2c9308edd53c6e33b3d4e453782c0063dc0028bdebaddf98              3 minutes ago  Up 3 minutes ago  127.0.0.1:55210-&gt;6443/tcp                    dev-control-plane\ndd461d2b9d4a  docker.io/kindest/node@sha256:adfaebada924a26c2c9308edd53c6e33b3d4e453782c0063dc0028bdebaddf98              3 minutes ago  Up 3 minutes ago  0.0.0.0:3080-&gt;80/tcp, 0.0.0.0:3443-&gt;443/tcp  dev-worker\n</code></pre></p> <ul> <li>See cluster up and running:</li> </ul> <p><pre><code>kubectl get nodes\n</code></pre> output<pre><code>NAME                STATUS   ROLES           AGE   VERSION\ndev-control-plane   Ready    control-plane   11h   v1.24.4\ndev-worker          Ready    &lt;none&gt;          11h   v1.24.4\n</code></pre></p> <p>Note</p> <p>You can see that our cluster has <code>control-plane</code> node and <code>worker</code> node.</p> <ul> <li>Verify k8s cluster status:</li> </ul> <pre><code>kubectl cluster-info\n</code></pre> <pre><code>Kubernetes control plane is running at https://127.0.0.1:56141\nCoreDNS is running at https://127.0.0.1:56141/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n</code></pre> <ul> <li>Check system Pods are up and running: </li> </ul> <p><pre><code>kubectl get pods -A\n</code></pre> <pre><code>NAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGE\nkube-system          coredns-6d4b75cb6d-lqcs6                     1/1     Running   0          4m6s\nkube-system          coredns-6d4b75cb6d-xgbxk                     1/1     Running   0          4m6s\nkube-system          etcd-dev-control-plane                       1/1     Running   0          4m18s\nkube-system          kindnet-tjfzj                                1/1     Running   0          4m6s\nkube-system          kindnet-vc66d                                1/1     Running   0          4m1s\nkube-system          kube-apiserver-dev-control-plane             1/1     Running   0          4m18s\nkube-system          kube-controller-manager-dev-control-plane    1/1     Running   0          4m18s\nkube-system          kube-proxy-5kp6d                             1/1     Running   0          4m6s\nkube-system          kube-proxy-dfczd                             1/1     Running   0          4m1s\nkube-system          kube-scheduler-dev-control-plane             1/1     Running   0          4m18s\nlocal-path-storage   local-path-provisioner-6b84c5c67f-csxg6      1/1     Running   0          4m6s\n</code></pre></p>"},{"location":"local_cluster/kind/#create-a-second-kind-cluster-stg","title":"Create a second kind cluster stg","text":"<p><pre><code>cat &gt; kind-stg.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nkubeadmConfigPatches:\n- |\n  apiVersion: kubeadm.k8s.io/v1beta2\n  kind: ClusterConfiguration\n  metadata:\n    name: config\n  apiServer:\n    extraArgs:\n      \"service-account-issuer\": \"kubernetes.default.svc\"\n      \"service-account-signing-key-file\": \"/etc/kubernetes/pki/sa.key\"\nnetworking:\n  # the default CNI will not be installed if you enable it, usefull to install Cilium !\n  disableDefaultCNI: false\nnodes:\n- role: control-plane\n  image: kindest/node:v1.25.2@sha256:9be91e9e9cdf116809841fc77ebdb8845443c4c72fe5218f3ae9eb57fdb4bace\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 4080\n    listenAddress: \"0.0.0.0\"\n  - containerPort: 443\n    hostPort: 4443\n    listenAddress: \"0.0.0.0\"\nEOF\n</code></pre> <pre><code>kind create cluster --name=stg --config kind-stg.yaml -v9 --retain\n</code></pre></p> <ul> <li>List clusters with <code>kind get</code>:</li> </ul> <p><pre><code>kind get clusters\n</code></pre> Output: <pre><code>dev\nstg\n</code></pre></p> <p>Result</p> <p>Two K8s clusters <code>dev</code> and <code>stg</code> has been created</p> <p>Note</p> <p>After a reboot, <code>podman</code> will be disabled. To recover podman and <code>kind</code> containers re-run following steps:</p> <pre><code>podman machine start\npodman start --all\n</code></pre>"},{"location":"local_cluster/kind/#next","title":"Next","text":"<p>Now that the <code>Kind</code> clusters are created, continue by deploying some applications.</p>"},{"location":"local_cluster/podman/","title":"Podman","text":"<p>Important</p> <p>If you already have Docker-Desktop or Docker Engine (Linux) installed and running, skip this step for the demo and go directly to Deploying applications with kubectl</p> <p><code>podman</code> is a full replacement of Docker and Docker-For-Desktop. It's the container Swiss-Army knife from RedHat.</p> <p>What you get with Podman:</p> <ul> <li>Multiple image format support, including the OCI and Docker image formats</li> <li>Full management of container lifecycle, Docker CLI replacement</li> <li>Container image management (managing image layers, overlay filesystems, etc)</li> <li>Podman version 3.4+ now support M1 Apple Macs</li> <li>Replaces Docker-for-Desktop and includes a UI</li> <li>no bundled Kubernetes, use kind, minikube, k3s, microk8s...</li> </ul> <p>It can also run and build rootless containers.</p>"},{"location":"local_cluster/podman/#install","title":"Install","text":"Apple Mac OsXLinux AlpineLinux CentosUbuntu / DebianWindows OsX Install<pre><code>brew install podman\n</code></pre> Linux Install<pre><code># Alpine\nsudo apk add podman\n</code></pre> Linux Install<pre><code># Centos\nsudo yum -y install podman\n</code></pre> Linux Install<pre><code># ubuntu / Debian\napt-get -y install podman\n</code></pre> <p>Windows specific</p> <p>On Windows, each Podman machine is backed by a virtualized Windows System for Linux (WSLv2) distribution. The podman command can be run directly from your Windows PowerShell (or CMD) prompt, where it remotely communicates with the podman service running in the WSL environment.</p> <p>Please check the docs for specific detailed instructions</p> <p>Optionally, Download Podman Desktop (UI) from the official website</p>"},{"location":"local_cluster/podman/#setup","title":"Setup","text":"<p>By default <code>Podman</code> set up a VM Machine of 1 CPU, 2Gb of memory, 100Gb of disk.</p> <p>In order to support demo needs create Podman VM with following parameters:</p> <pre><code>podman machine init \\\n--cpus=2 \\\n--memory=4096 \\\n--disk-size=200 \\\n--now\n\n# podman machine start # Not required because of --now option\n</code></pre> <p>Check Podman is running:</p> <pre><code>podman info\n</code></pre> <p>Make Docker command call Podman, Podman is command-line compatible with Docker:</p> <pre><code>mv -f /usr/local/bin/docker /usr/local/bin/docker-orig\nln -s /usr/local/bin/podman /usr/local/bin/docker\n</code></pre> <p>Point the default Docker socket to the Podman socket. This is needed as some apps use a hardcoded path to Docker:</p> <pre><code># This is needed so every app \"hardcoded\" for Docker will work\nexport DOCKER_HOST=\"unix://$HOME/.local/share/containers/podman/machine/podman-machine-default/podman.sock\"\n</code></pre>"},{"location":"local_cluster/podman/#usage","title":"Usage","text":"<p>You can use podman to search for well-known images: </p> <pre><code>podman search httpd\n</code></pre> <p>You can run an image with the same command as with <code>docker</code>:</p> <pre><code>podman run -d alpine:latest sleep 20\n</code></pre> <p>Then, list the running containers multiple times:</p> <p><pre><code>podman ps -a\n</code></pre> output<pre><code>CONTAINER ID  IMAGE                             COMMAND     CREATED      STATUS                 PORTS     NAMES\nc4b74e45f004  docker.io/library/alpine:latest   sleep 20    2 hours ago  Up 2 hours ago                   loving_wu\n</code></pre></p> <p>You can also use the <code>docker</code> command, as it's executing <code>podman</code> in the background, and <code>podman</code> support all the same arguments:</p> <p><pre><code>docker ps -a\n</code></pre> output<pre><code>CONTAINER ID  IMAGE                             COMMAND     CREATED      STATUS                 PORTS     NAMES\nc4b74e45f004  docker.io/library/alpine:latest   sleep 20    2 hours ago  Exited (0) 2 hours ago\n</code></pre></p>"},{"location":"local_cluster/podman/#tips-and-tricks","title":"Tips and Tricks","text":"<p>You can also use Podman to convert a running docker image into a Kubernetes yaml using:</p> <pre><code>podman generate kube my-running-app -f ~/my-running-app.yaml\n</code></pre> <p>You can also convert a yaml file back to bunch of containers run in Podman:</p> <pre><code>podman play kube /mnt/mysharedfolder/my-running-app.yaml\n</code></pre> <p>Warning</p> <p>Sometimes some pods way complaine with <code>failed to create fsnotify watcher: too many open files</code>.</p> <p>This is due to the tuning of the <code>machine</code> default values that are too low. Edit the machine:</p> <pre><code>podman machine ssh\nsudo -s\n</code></pre> <p>then add those 2 lines in <code>/etc/sysctl.conf</code>:</p> <pre><code>fs.inotify.max_user_instances = 10240\nfs.inotify.max_user_watches = 122880\n</code></pre> <p>And execute:</p> <pre><code>sysctl -w fs.inotify.max_user_instances=10240\nsysctl -w fs.inotify.max_user_watches=122880\n</code></pre>"},{"location":"local_cluster/podman/#next","title":"Next","text":"<p>Podman is a cool alternative to Docker Engine and Docker CLI. </p> <p>However, Podman does not provide a K8s cluster.</p> <p>Create a local <code>Kind</code> Kubernetes cluster in next chapter !</p>"},{"location":"local_cluster/options/colima/","title":"Colima as Docker Desktop replacement for MAC OS","text":"<p>Important</p> <p>This step is not executed as part of the tutorial it's only a reciepes how to replace Docker for Desktop</p> <p>Colima is a full Docker-Desktop replacement. It is specific to Mac OSX. Just use plain Docker or Containerd on Linux.</p> <p>With Colima you get:</p> <ul> <li>Intel and M1 Macs support</li> <li>Simple CLI interface</li> <li>Both Docker and Containerd support</li> <li>Port Forwarding</li> <li>Volume mounts</li> <li>Bundeled Kubernetes cluster</li> <li>A full replacement of Docker-for-Desktop</li> </ul>"},{"location":"local_cluster/options/colima/#install","title":"Install","text":"<pre><code>brew install colima\nbrew install docker # if you're using the Docker shim\n</code></pre> <p>Note</p> <p>Install <code>Colima</code> version <code>v0.4.6</code> or newer, which solves a bug with Kubernetes version reset (see https://github.com/abiosoft/colima/issues/417)</p>"},{"location":"local_cluster/options/colima/#usage","title":"Usage","text":"<p>Colima Config file</p> <p><code>Colima</code> uses a config file, located by default in <code>$HOME/.colima/default/colima.yaml</code>.</p> <p>We're not going to dive into it, but you can update this file to change some of the parameters instead of specifying it on the command-line. </p> <p>This file is auto-created when you first start Colima.</p> <pre><code>colima start --runtime containerd --with-kubernetes --kubernetes-version v1.24.4+k3s1\ncolima status\n\n# other start options not used for this tutorial\ncolima start                                         # default using Docker runtime\ncolima start --with-kubernetes                       # start kubernetes local cluster\ncolima start --runtime containerd --with-kubernetes  # remove docker completely\n</code></pre> <p>Once Colima is running we can start using it. </p> <p>Container Interface</p> <p>Because we used <code>containerd</code>, we have to use the <code>nerdctl</code> command instead of <code>docker</code>. </p> <p><code>nerdctl</code> is installed by Colima itself.</p> <p>Run a container for 20 seconds:</p> <pre><code>nerdctl run -d alpine:latest sleep 20\n\ncolima nerdctl ps # list running containers, execute multiple times to see the container stop after 10s\n</code></pre> <p>We started Colima with a <code>kubernetes</code> cluster. By default Colima will update your <code>kubeconfig</code>  to add this cluster and set this new context as default.</p> <pre><code>kubectl config get-contexts\n\nkubectl get pods -A\n\nkubectl version\n</code></pre>"},{"location":"local_cluster/options/colima/#stop","title":"Stop","text":"<p>For the rest of the presentation we're not going to use Colima. You can stop it:</p> <pre><code>colima kubernetes delete # stop k8s and delete the associated files\ncolima stop\n</code></pre>"},{"location":"local_cluster/options/minikube/","title":"MiniKube as Docker Desktop replacement","text":"<p>Important</p> <p>This step is not executed as part of the tutorial it's only a reciepes how to replace Docker for Desktop</p> <p>Minikube is local Kubernetes, focusing on making it easy to learn and develop for Kubernetes.</p> <p>All you need is Docker (or similarly compatible) container or a Virtual Machine environment</p> <ul> <li>Well documented</li> <li>Can use many container Runtime (get rid of Docker !)</li> </ul> <p>https://minikube.sigs.k8s.io/docs/start/</p>"},{"location":"local_cluster/options/minikube/#install-docker-cli","title":"Install Docker CLI","text":"<pre><code>brew install docker\n</code></pre>"},{"location":"local_cluster/options/minikube/#install-minikube","title":"Install Minikube","text":"Apple Mac OsXLinuxWindows OsX Install<pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64\nsudo install minikube-darwin-amd64 /usr/local/bin/minikube\n</code></pre> Linux Install<pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n</code></pre> Windows Install<pre><code>New-Item -Path 'c:\\' -Name 'minikube' -ItemType Directory -Force\nInvoke-WebRequest -OutFile 'c:\\minikube\\minikube.exe' -Uri 'https://github.com/kubernetes/minikube/releases/latest/download/minikube-windows-amd64.exe' -UseBasicParsing\n</code></pre> <p>Install Matrix</p> <p>Refer to the Installation Matrix to get the right command, like installing on Arm64 CPU.</p>"},{"location":"local_cluster/options/minikube/#start-docker-with-hyperkit-httpsminikubesigsk8siodocsdrivershyperkit","title":"Start Docker with HyperKit (https://minikube.sigs.k8s.io/docs/drivers/hyperkit/)","text":"<pre><code>minikube start --driver hyperkit # will be auto-detected\n</code></pre>"},{"location":"local_cluster/options/minikube/#point-docker-cli-to-docker-engine-inside-minikube-vm","title":"Point Docker CLI to Docker Engine inside Minikube VM","text":"<pre><code>minikube docker-env\neval $(minikube docker-env)\n</code></pre> <p>Show Cluster info</p> <pre><code>kubectl cluster-info\n</code></pre> <pre><code>Kubernetes control plane is running at https://192.168.64.2:8443\nCoreDNS is running at https://192.168.64.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n</code></pre> <ul> <li>Running pods</li> </ul> <p><pre><code>kubectl get po -A\n</code></pre> <pre><code>NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE\nkube-system   coredns-565d847f94-2mx8q           1/1     Running   0          109s\nkube-system   etcd-minikube                      1/1     Running   0          2m2s\nkube-system   kube-apiserver-minikube            1/1     Running   0          2m2s\nkube-system   kube-controller-manager-minikube   1/1     Running   0          2m2s\nkube-system   kube-proxy-rcklg                   1/1     Running   0          109s\nkube-system   kube-scheduler-minikube            1/1     Running   0          2m2s\nkube-system   storage-provisioner                1/1     Running   0          2m1s\n</code></pre></p> <ul> <li>All k8s contexts</li> </ul> <p>We can check the current Kubernetes <code>context</code> using <code>kubectl</code>:</p> <p><pre><code>k config  get-contexts\n</code></pre> <pre><code>CURRENT   NAME        CLUSTER     AUTHINFO    NAMESPACE\n          kind-demo   kind-demo   kind-demo\n*         minikube    minikube    minikube    default\n</code></pre></p>"},{"location":"local_cluster/options/rancher/","title":"Rancher Desktop as Docker Desktop replacement","text":"<p>Important</p> <p>This step is not executed as part of the tutorial it's only a reciepes how to replace Docker for Desktop</p> <p>Rancher Desktop an open-source desktop application for Mac, Windows and Linux. Rancher Desktop runs Kubernetes and container management on your desktop. You can choose the version of Kubernetes you want to run. You can build, push, pull, and run container images using either containerd or Moby (dockerd). The container images you build can be run by Kubernetes immediately without the need for a registry.</p>"},{"location":"local_cluster/options/rancher/#install-rancher-desktop","title":"Install Rancher Desktop","text":"<p>Follow the instructions to setup Rancher Desktop for preferred OS.</p>"},{"location":"local_cluster/options/rancher/#configure-nerdctl","title":"Configure nerdctl","text":"<p><code>nerdctl</code> is a Docker-compatible CLI for containerd:</p> <ul> <li>Supports rootless mode</li> <li>Supports lazy-pulling</li> <li>Supports Docker Compose (nerdctl compose up)</li> </ul> <p><code>nerdctl</code> get installed as part of Rancher Desktop and can be configured to replace docker command:</p> <pre><code>nerdctl help\n\nalias docker=nerdctl\n</code></pre> <p>That should be added to <code>~/.bashrc</code> or <code>~/.zshrc</code></p> <pre><code>docker help\n</code></pre>"},{"location":"local_cluster/options/rancher/#run-containers-with-nerdctl","title":"Run Containers with nerdctl","text":"<pre><code>docker container ls\n\ndocker container run --rm -it \\\n    alpine echo \"Is it working?\"\n</code></pre>"},{"location":"local_cluster/options/rancher/#docker-compose-with-nerdctl","title":"Docker Compose with nerdctl","text":"<pre><code>docker compose up --detach\n\ndocker container ls\n\ndocker compose down\n</code></pre>"},{"location":"local_cluster/options/rancher/#container-images-with-nerdctl","title":"Container Images with nerdctl","text":"<pre><code>docker image build \\\n    --tag $DH_USER/devops-toolkit \\\n    .\n\ndocker login \\\n    --username $DH_USER \\\n    --password $DH_PASS\n\ndocker image push $DH_USER/devops-toolkit\n\ndocker image tag \\\n    $DH_USER/devops-toolkit \\\n    $DH_USER/devops-toolkit:0.0.1\n\ndocker image ls\n</code></pre>"},{"location":"yaml/","title":"Readme","text":""},{"location":"yaml/#building-the-docker-images-of-the-gowebapp","title":"Building the Docker images of the GoWebApp","text":"<pre><code>git clone git@github.com:Cloud-Architects-Program/ycit019_2022.git\ncd ycit019_2022/Mod8_assignment/gowebapp\n</code></pre> <p>Change the Dockerfile to:</p> <pre><code>FROM golang:1.16.4 as build\n\nENV GO111MODULE=auto\nENV GOPATH=/go\n\nCOPY /code $GOPATH/src/gowebapp/\nWORKDIR $GOPATH/src/gowebapp/\nRUN go get &amp;&amp; CGO_ENABLED=0 go build -o /go/bin/gowebapp\n\nFROM gcr.io/distroless/static\n\nUSER nonroot:nonroot\nLABEL maintainer \"student@mcgill.ca\"\nLABEL gowebapp \"v1\"\nENV DB_PASSWORD=rootpasswd\n\nEXPOSE 80\nCOPY --from=build --chown=nonroot:nonroot /go/bin/gowebapp /gowebapp\nCOPY --from=build --chown=nonroot:nonroot /go/src/gowebapp/config /config\nCOPY --from=build --chown=nonroot:nonroot /go/src/gowebapp/template /template\nCOPY --from=build --chown=nonroot:nonroot /go/src/gowebapp/static /static\nENTRYPOINT [\"/gowebapp\"]\n</code></pre> <p>Update config in <code>code/config/config.json</code> and change port to 9000:</p> <pre><code>    \"Server\": {\n        \"Hostname\": \"\",\n        \"UseHTTP\": true,\n        \"UseHTTPS\": false,\n        \"HTTPPort\": 9000,\n        \"HTTPSPort\": 9443,\n        \"CertFile\": \"tls/server.crt\",\n        \"KeyFile\": \"tls/server.key\"\n    },\n</code></pre> <p>then build / tag / push</p> <pre><code>podman build -t cloud-native-canada/k8s_setup_tools:app .\npodman tag localhost/cloud-native-canada/k8s_setup_tools:app ghcr.io/cloud-native-canada/k8s_setup_tools:app\ndocker push ghcr.io/cloud-native-canada/k8s_setup_tools:app\n</code></pre>"}]}